{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 07. [DL 입문] - 순환 신경망(Recurrent Neural Network)\n",
    "\n",
    "cell : hidden layer에서 activation function을 통해 결과를 내보내는 역할\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "# 파이썬으로 RNN 구현\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "timesteps = 10  # 시점의 수. NLP 에서 보통 문장의 길이가 된다.\n",
    "input_size = 4  # 입력의 차원. NLP 에서 보통 단어 벡터의 차원이 된다.\n",
    "hidden_size = 8  # 은닉 상태의 크기. 메모리 셀의 용량이다.\n",
    "\n",
    "inputs = np.random.random((timesteps, input_size))  # 입력에 해당되는 2D 텐서\n",
    "\n",
    "hidden_state_t = np.zeros((hidden_size,))  # 초기 은닉 상태는 0 vector로 초기화\n",
    "# 은닉 상태의 크기 hidden_size 로 은닉 상태를 만듬.\n",
    "\n",
    "print(hidden_state_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8, 4)\n",
      "(8, 8)\n",
      "(8,)\n"
     ]
    }
   ],
   "source": [
    "Wx = np.random.random(\n",
    "    (hidden_size, input_size)\n",
    ")  # (8,4) 크기의 2D 텐서 생성, 입력에 대한 가중치.\n",
    "Wh = np.random.random(\n",
    "    (hidden_size, hidden_size)\n",
    ")  # (8,8) 크기의 2D 텐서 생성, 은닉 상태에 대한 가중치.\n",
    "b = np.random.random((hidden_size,))  # (8,) 크기의 1D 텐서 생성. 이 값은 편향(bias)\n",
    "\n",
    "print(np.shape(Wx))\n",
    "print(np.shape(Wh))\n",
    "print(np.shape(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 8)\n",
      "(2, 8)\n",
      "(3, 8)\n",
      "(4, 8)\n",
      "(5, 8)\n",
      "(6, 8)\n",
      "(7, 8)\n",
      "(8, 8)\n",
      "(9, 8)\n",
      "(10, 8)\n",
      "[[0.92024635 0.93138331 0.90075012 0.69291843 0.95127308 0.70995549\n",
      "  0.90558744 0.95885697]\n",
      " [0.99999054 0.99995118 0.99998381 0.99993691 0.99998537 0.99969607\n",
      "  0.99998104 0.99995659]\n",
      " [0.9999919  0.99996501 0.99998543 0.99996571 0.99999187 0.9998793\n",
      "  0.99999649 0.99994578]\n",
      " [0.99999185 0.99996324 0.99998922 0.99996165 0.99999088 0.99987623\n",
      "  0.99999146 0.99995237]\n",
      " [0.99999539 0.99998016 0.99999108 0.99997223 0.99999731 0.9999518\n",
      "  0.99999631 0.99997403]\n",
      " [0.9999909  0.99997012 0.99999303 0.99995095 0.99999382 0.99992077\n",
      "  0.99999065 0.99996187]\n",
      " [0.99999505 0.99998777 0.99999629 0.99997813 0.99999762 0.99993667\n",
      "  0.9999948  0.99998438]\n",
      " [0.99998703 0.99995423 0.99998959 0.99994436 0.99998636 0.99984816\n",
      "  0.99999067 0.9999353 ]\n",
      " [0.99999604 0.99998171 0.99999065 0.99997586 0.99999767 0.99995476\n",
      "  0.99999683 0.99997612]\n",
      " [0.99999345 0.99997514 0.99999258 0.99997637 0.99999222 0.99983736\n",
      "  0.99999197 0.99996803]]\n"
     ]
    }
   ],
   "source": [
    "total_hidden_states = []\n",
    "\n",
    "# 메모리 셀 동작\n",
    "for input_t in inputs:  # 각 시점에 따라서 입력값이 입력됨.\n",
    "    output_t = np.tanh(np.dot(Wx, input_t) + np.dot(Wh, hidden_state_t) + b)\n",
    "    total_hidden_states.append(\n",
    "        list(output_t)\n",
    "    )  # 각 시점의 은닉 상태의 값을 계속해서 축적\n",
    "    print(\n",
    "        np.shape(total_hidden_states)\n",
    "    )  # 각 시점 t별 메모리 셀의 출력의 크기는 (timestep, output_dim)\n",
    "    hidden_state_t = output_t\n",
    "\n",
    "total_hidden_states = np.stack(total_hidden_states, axis=0)\n",
    "# 출력시 값을 깔끔하게 해준다.\n",
    "\n",
    "print(\n",
    "    total_hidden_states\n",
    ")  # (timesteps, output_dim)의 크기. 이 경우 (10,8)의 크기를 가지는 메모리 셀의 2D 텐서를 출력."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파이토치의 nn.RNN()\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 10, 8])\n",
      "torch.Size([1, 1, 8])\n"
     ]
    }
   ],
   "source": [
    "input_size = 5  # 입력의 크기 -> 매 시점마다 들어가는 입력의 크기\n",
    "hidden_size = 8  # 은닉 상태의 크기\n",
    "\n",
    "# 입력 텐서\n",
    "# (batch_size, time_steps, input_size)\n",
    "inputs = torch.Tensor(1, 10, 5)\n",
    "\n",
    "cell = nn.RNN(\n",
    "    input_size, hidden_size, batch_first=True\n",
    ")  # 입력 텐서의 첫번째 차원이 배치 크기\n",
    "\n",
    "outputs, _status = cell(\n",
    "    inputs\n",
    ")  # outputs : 모든 시점의 은닉 상태, _status : 마지막 시점 은닉 상태\n",
    "\n",
    "print(outputs.shape)  # 모든 time-step 의 hidden_state\n",
    "print(_status.shape)  # 최종 time-step의 hidden_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 10, 8])\n",
      "torch.Size([2, 1, 8])\n"
     ]
    }
   ],
   "source": [
    "# Deep RNN\n",
    "\n",
    "# (batch_size, time_steps, input_size)\n",
    "inputs = torch.Tensor(1, 10, 5)\n",
    "\n",
    "cell = nn.RNN(input_size=5, hidden_size=8, num_layers=2, batch_first=True)\n",
    "outputs, _status = cell(inputs)\n",
    "\n",
    "print(outputs.shape)\n",
    "print(_status.shape)  # (층의 갯수, 배치 크기, 은닉 상태의 크기)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 10, 16])\n",
      "torch.Size([4, 1, 8])\n"
     ]
    }
   ],
   "source": [
    "# Bidirectional RNN\n",
    "\n",
    "# (batch_size, time_steps, input_size)\n",
    "inputs = torch.Tensor(1, 10, 5)\n",
    "\n",
    "cell = nn.RNN(\n",
    "    input_size=5, hidden_size=8, num_layers=2, batch_first=True, bidirectional=True\n",
    ")\n",
    "outputs, _status = cell(inputs)\n",
    "\n",
    "print(outputs.shape)  # 배치 크기, 시퀀스 길이, 은닉 상태의 크기 x 2\n",
    "print(_status.shape)  # (층의 개수 x2, 배치 크기, 은닉 상태의 크기)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM과 GRU\n",
    "- 바닐라 RNN의 한계 : time-step이 길어질수록 앞의 정보가 뒤로 충분히 전달되지 못함\n",
    "- 장기 의존성 문제(the problem of Long-Term Dependencies)\n",
    "\n",
    "### LSTM\n",
    "- Long SHort-Term Memory\n",
    "1. 입력 게이트\n",
    "    - 현재의 정보를 기억하기 위한 게이트\n",
    "2. 삭제 게이트\n",
    "    - 기억을 삭제하기 위한 게이트\n",
    "3. 셀 상태(장기 상태)\n",
    "    - 입력 게이트와 삭제 게이트의 조합\n",
    "4. 출력 게이트와 은닉 상태(단기 상태)\n",
    "    \n",
    "### GRU\n",
    "1. 업데이트 게이트\n",
    "2. 리셋 게이트\n",
    "\n",
    "LSTM, GRU 성능은 비슷하다고 알려짐\n",
    "경험적으로 데이터 양이 ㅈ거을 때는 매개변수의 양이 적은 GRU가 조금 낫고\n",
    "데이터 양이 많으면 LSTM이 더 낫다고도 함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파이토치의 nn.LSTM()\n",
    "# nn.LSTM(input_dim, hidden_size, batch_first = True)\n",
    "\n",
    "# 파이토치의 nn.GRU()\n",
    "# nn.GRU(input_dim, hidden_size, batch_first = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문자 단위 RNN (Char RNN)\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "문자 집합의 크기 : 5\n"
     ]
    }
   ],
   "source": [
    "# 1.훈련 데이터 전처리하기\n",
    "# apple -> pple!\n",
    "\n",
    "input_str = \"apple\"\n",
    "label_str = \"pple!\"\n",
    "char_vocab = sorted(list(set(input_str + label_str)))\n",
    "vocab_size = len(char_vocab)\n",
    "\n",
    "print(f\"문자 집합의 크기 : {vocab_size}\")  # !,a,e,l,p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = vocab_size  # 입력의 크기는 문자 집합의 크기, 원핫벡터사용\n",
    "hidden_size = 5\n",
    "output_size = 5\n",
    "learning_rate = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'!': 0, 'a': 1, 'e': 2, 'l': 3, 'p': 4}\n"
     ]
    }
   ],
   "source": [
    "char_to_index = dict((c,i) for i,c in enumerate(char_vocab)) # 문자에 고유한 정수 인덱스 부여\n",
    "print(char_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: '!', 1: 'a', 2: 'e', 3: 'l', 4: 'p'}\n"
     ]
    }
   ],
   "source": [
    "# 예측 결과를 다시 문자 시퀀스로 보기 위해 정수로부터 문자를 얻을 수 있는 index_to_char\n",
    "index_to_char = {}\n",
    "for key, value in char_to_index.items():\n",
    "    index_to_char[value] = key\n",
    "print(index_to_char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 4, 4, 3, 2]\n",
      "[4, 4, 3, 2, 0]\n"
     ]
    }
   ],
   "source": [
    "# 입력 데이터와 레이블 데이터의 각 문자들을 정수로 맵핑\n",
    "x_data = [char_to_index[c] for c in input_str]\n",
    "y_data = [char_to_index[c] for c in label_str]\n",
    "print(x_data)\n",
    "print(y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 4, 4, 3, 2]]\n",
      "[[4, 4, 3, 2, 0]]\n"
     ]
    }
   ],
   "source": [
    "# 파이토치 nn.RNN()은 기본적으로 3차원 텐서 받음\n",
    "# 배치 차원 추가\n",
    "# 텐서 연산인 unsqueeze(0)을 통해 해결할 수도 있었음.\n",
    "x_data = [x_data]\n",
    "y_data =[y_data]\n",
    "print(x_data)\n",
    "print(y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[0., 1., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 1.],\n",
      "       [0., 0., 0., 0., 1.],\n",
      "       [0., 0., 0., 1., 0.],\n",
      "       [0., 0., 1., 0., 0.]])]\n"
     ]
    }
   ],
   "source": [
    "# 입력 시퀀스의 각 문자들을 원-핫 벡터로 바꿔줌\n",
    "x_one_hot = [np.eye(vocab_size)[x] for x in x_data]\n",
    "print(x_one_hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "훈련 데이터의 크기 : torch.Size([1, 5, 5])\n",
      "레이블의 크기 : torch.Size([1, 5])\n"
     ]
    }
   ],
   "source": [
    "# 입력 데이터와 레이블 데이터를 텐서로 바꿔줌\n",
    "X = torch.FloatTensor(x_one_hot)\n",
    "y = torch.LongTensor(y_data)\n",
    "\n",
    "# 각 텐서 크기 확인\n",
    "print(f'훈련 데이터의 크기 : {X.shape}')\n",
    "print(f'레이블의 크기 : {y.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 5, 5])\n",
      "torch.Size([5, 5])\n",
      "torch.Size([1, 5])\n",
      "torch.Size([5])\n"
     ]
    }
   ],
   "source": [
    "# 모델 구현\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(Net, self).__init__()\n",
    "        self.rnn = torch.nn.RNN(input_size, hidden_size, batch_first = True) # RNN 셀 구현\n",
    "        self.fc = torch.nn.Linear(hidden_size, output_size, bias=True) # 출력층 구현\n",
    "    \n",
    "    def forward(self, x): # 구현한 RNN 셀과 출력층을 연결\n",
    "        x, _status = self.rnn(x)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "    \n",
    "net = Net(input_size, hidden_size, output_size)\n",
    "\n",
    "outputs = net(X)\n",
    "\n",
    "print(outputs.shape) # 3차원 텐서\n",
    "print(outputs.view(-1, input_size).shape) # 2차원 텐서로 변환\n",
    "print(y.shape)\n",
    "print(y.view(-1).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 loss:  1.4957239627838135 prediction:  [[4 4 2 0 0]] true y:  [[4, 4, 3, 2, 0]] prediction str:  ppe!!\n",
      "1 loss:  1.2863942384719849 prediction:  [[4 3 3 2 2]] true y:  [[4, 4, 3, 2, 0]] prediction str:  pllee\n",
      "2 loss:  1.0699931383132935 prediction:  [[4 4 3 2 2]] true y:  [[4, 4, 3, 2, 0]] prediction str:  pplee\n",
      "3 loss:  0.856244683265686 prediction:  [[4 4 4 2 0]] true y:  [[4, 4, 3, 2, 0]] prediction str:  pppe!\n",
      "4 loss:  0.6447590589523315 prediction:  [[4 4 3 2 0]] true y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "5 loss:  0.4812183976173401 prediction:  [[4 4 3 2 0]] true y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "6 loss:  0.37004247307777405 prediction:  [[4 4 3 2 0]] true y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "7 loss:  0.2730952799320221 prediction:  [[4 4 3 2 0]] true y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "8 loss:  0.1985623836517334 prediction:  [[4 4 3 2 0]] true y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "9 loss:  0.14216120541095734 prediction:  [[4 4 3 2 0]] true y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "10 loss:  0.0987517386674881 prediction:  [[4 4 3 2 0]] true y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "11 loss:  0.06745387613773346 prediction:  [[4 4 3 2 0]] true y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "12 loss:  0.04710625112056732 prediction:  [[4 4 3 2 0]] true y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "13 loss:  0.03443664312362671 prediction:  [[4 4 3 2 0]] true y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "14 loss:  0.026335736736655235 prediction:  [[4 4 3 2 0]] true y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "15 loss:  0.020876752212643623 prediction:  [[4 4 3 2 0]] true y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "16 loss:  0.01698322407901287 prediction:  [[4 4 3 2 0]] true y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "17 loss:  0.014057028107345104 prediction:  [[4 4 3 2 0]] true y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "18 loss:  0.011766182258725166 prediction:  [[4 4 3 2 0]] true y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "19 loss:  0.009928114712238312 prediction:  [[4 4 3 2 0]] true y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "20 loss:  0.008438251912593842 prediction:  [[4 4 3 2 0]] true y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "21 loss:  0.00722885224968195 prediction:  [[4 4 3 2 0]] true y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "22 loss:  0.006247988902032375 prediction:  [[4 4 3 2 0]] true y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "23 loss:  0.005452098324894905 prediction:  [[4 4 3 2 0]] true y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "24 loss:  0.004804156720638275 prediction:  [[4 4 3 2 0]] true y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "25 loss:  0.004273506347090006 prediction:  [[4 4 3 2 0]] true y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "26 loss:  0.00383562920615077 prediction:  [[4 4 3 2 0]] true y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "27 loss:  0.0034711305052042007 prediction:  [[4 4 3 2 0]] true y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "28 loss:  0.003165092784911394 prediction:  [[4 4 3 2 0]] true y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "29 loss:  0.0029059338849037886 prediction:  [[4 4 3 2 0]] true y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "30 loss:  0.0026846148539334536 prediction:  [[4 4 3 2 0]] true y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "31 loss:  0.0024943642783910036 prediction:  [[4 4 3 2 0]] true y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "32 loss:  0.0023294708225876093 prediction:  [[4 4 3 2 0]] true y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "33 loss:  0.0021857856772840023 prediction:  [[4 4 3 2 0]] true y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "34 loss:  0.002059822902083397 prediction:  [[4 4 3 2 0]] true y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "35 loss:  0.001948853605426848 prediction:  [[4 4 3 2 0]] true y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "36 loss:  0.0018505515763536096 prediction:  [[4 4 3 2 0]] true y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "37 loss:  0.001763088977895677 prediction:  [[4 4 3 2 0]] true y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "38 loss:  0.0016849696403369308 prediction:  [[4 4 3 2 0]] true y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "39 loss:  0.0016148872673511505 prediction:  [[4 4 3 2 0]] true y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "40 loss:  0.0015517963329330087 prediction:  [[4 4 3 2 0]] true y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "41 loss:  0.0014948417665436864 prediction:  [[4 4 3 2 0]] true y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "42 loss:  0.0014432150637730956 prediction:  [[4 4 3 2 0]] true y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "43 loss:  0.0013962745433673263 prediction:  [[4 4 3 2 0]] true y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "44 loss:  0.0013534501194953918 prediction:  [[4 4 3 2 0]] true y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "45 loss:  0.0013142895186319947 prediction:  [[4 4 3 2 0]] true y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "46 loss:  0.0012784363934770226 prediction:  [[4 4 3 2 0]] true y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "47 loss:  0.0012453910894691944 prediction:  [[4 4 3 2 0]] true y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "48 loss:  0.0012149157701060176 prediction:  [[4 4 3 2 0]] true y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "49 loss:  0.0011868443107232451 prediction:  [[4 4 3 2 0]] true y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "50 loss:  0.0011607480701059103 prediction:  [[4 4 3 2 0]] true y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "51 loss:  0.0011365083046257496 prediction:  [[4 4 3 2 0]] true y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "52 loss:  0.0011139585403725505 prediction:  [[4 4 3 2 0]] true y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "53 loss:  0.0010929559357464314 prediction:  [[4 4 3 2 0]] true y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "54 loss:  0.0010732149239629507 prediction:  [[4 4 3 2 0]] true y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "55 loss:  0.0010547831188887358 prediction:  [[4 4 3 2 0]] true y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "56 loss:  0.0010374225676059723 prediction:  [[4 4 3 2 0]] true y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "57 loss:  0.001021085656248033 prediction:  [[4 4 3 2 0]] true y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "58 loss:  0.0010056771570816636 prediction:  [[4 4 3 2 0]] true y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "59 loss:  0.0009911258239299059 prediction:  [[4 4 3 2 0]] true y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "60 loss:  0.0009772885823622346 prediction:  [[4 4 3 2 0]] true y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "61 loss:  0.0009640941279940307 prediction:  [[4 4 3 2 0]] true y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "62 loss:  0.0009516378631815314 prediction:  [[4 4 3 2 0]] true y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "63 loss:  0.0009397054091095924 prediction:  [[4 4 3 2 0]] true y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "64 loss:  0.0009283204562962055 prediction:  [[4 4 3 2 0]] true y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "65 loss:  0.0009174593724310398 prediction:  [[4 4 3 2 0]] true y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "66 loss:  0.0009070031228475273 prediction:  [[4 4 3 2 0]] true y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "67 loss:  0.0008969752816483378 prediction:  [[4 4 3 2 0]] true y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "68 loss:  0.0008873285842128098 prediction:  [[4 4 3 2 0]] true y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "69 loss:  0.0008779674535617232 prediction:  [[4 4 3 2 0]] true y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "70 loss:  0.0008690351387485862 prediction:  [[4 4 3 2 0]] true y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "71 loss:  0.0008603407186456025 prediction:  [[4 4 3 2 0]] true y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "72 loss:  0.0008519560215063393 prediction:  [[4 4 3 2 0]] true y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "73 loss:  0.000843833084218204 prediction:  [[4 4 3 2 0]] true y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "74 loss:  0.0008359007770195603 prediction:  [[4 4 3 2 0]] true y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "75 loss:  0.0008282065391540527 prediction:  [[4 4 3 2 0]] true y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "76 loss:  0.0008207742357626557 prediction:  [[4 4 3 2 0]] true y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "77 loss:  0.0008134609088301659 prediction:  [[4 4 3 2 0]] true y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "78 loss:  0.0008062905399128795 prediction:  [[4 4 3 2 0]] true y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "79 loss:  0.0007994058541953564 prediction:  [[4 4 3 2 0]] true y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "80 loss:  0.000792640377767384 prediction:  [[4 4 3 2 0]] true y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "81 loss:  0.0007860176265239716 prediction:  [[4 4 3 2 0]] true y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "82 loss:  0.0007795140263624489 prediction:  [[4 4 3 2 0]] true y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "83 loss:  0.0007731771329417825 prediction:  [[4 4 3 2 0]] true y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "84 loss:  0.0007668877951800823 prediction:  [[4 4 3 2 0]] true y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "85 loss:  0.0007607413572259247 prediction:  [[4 4 3 2 0]] true y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "86 loss:  0.0007546901470050216 prediction:  [[4 4 3 2 0]] true y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "87 loss:  0.0007488293922506273 prediction:  [[4 4 3 2 0]] true y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "88 loss:  0.0007429687539115548 prediction:  [[4 4 3 2 0]] true y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "89 loss:  0.0007372986292466521 prediction:  [[4 4 3 2 0]] true y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "90 loss:  0.0007316760602407157 prediction:  [[4 4 3 2 0]] true y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "91 loss:  0.0007261725841090083 prediction:  [[4 4 3 2 0]] true y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "92 loss:  0.0007206691079773009 prediction:  [[4 4 3 2 0]] true y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "93 loss:  0.0007153084734454751 prediction:  [[4 4 3 2 0]] true y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "94 loss:  0.000710019376128912 prediction:  [[4 4 3 2 0]] true y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "95 loss:  0.0007048016414046288 prediction:  [[4 4 3 2 0]] true y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "96 loss:  0.0006996315205469728 prediction:  [[4 4 3 2 0]] true y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "97 loss:  0.0006945091299712658 prediction:  [[4 4 3 2 0]] true y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "98 loss:  0.000689505715854466 prediction:  [[4 4 3 2 0]] true y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "99 loss:  0.0006844784948043525 prediction:  [[4 4 3 2 0]] true y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n"
     ]
    }
   ],
   "source": [
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(net.parameters(), learning_rate)\n",
    "\n",
    "for i in range(100):\n",
    "    optimizer.zero_grad()\n",
    "    outputs = net(X)\n",
    "    loss = criterion(outputs.view(-1, input_size), y.view(-1)) # view 하는 이유는 batch 차원 제거 위해\n",
    "    loss.backward() # 기울기 계산\n",
    "    optimizer.step()\n",
    "\n",
    "    # 모델이 실제로 어떻게 예측했는지를 확인하기 위한 코드\n",
    "    result = outputs.data.numpy().argmax(axis=2) # 최종 예측값인 각 time-step 별 5차원 벡터에 대해서 가장 높은 값의 인덱스를 선택\n",
    "    result_str = ''.join([index_to_char[c] for c in np.squeeze(result)])\n",
    "    print(i, \"loss: \", loss.item(), \"prediction: \", result, \"true y: \", y_data, \"prediction str: \", result_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{',': 0, 'l': 1, 'o': 2, 'g': 3, 'h': 4, 'n': 5, 'u': 6, 'm': 7, 's': 8, 'w': 9, 'a': 10, 'i': 11, 'c': 12, 'p': 13, 'e': 14, 'y': 15, \"'\": 16, 'd': 17, '.': 18, 'f': 19, 't': 20, ' ': 21, 'k': 22, 'r': 23, 'b': 24}\n",
      "문자 집합의 크기 : 25\n"
     ]
    }
   ],
   "source": [
    "# 더 많은 데이터로 학습한 문자 단위 RNN\n",
    "\n",
    "# 훈련 데이터 전처리\n",
    "\n",
    "sentence = (\"if you want to build a ship, don't drum up people together to \"\n",
    "            \"collect wood and don't assign them tasks and work, but rather \"\n",
    "            \"teach them to long for the endless immensity of the sea.\")\n",
    "\n",
    "char_set = list(set(sentence)) # 중복 제거한 문자 집합 생성\n",
    "char_dic = {c : i for i, c in enumerate(char_set)} # 각 문자에 정수 인코딩\n",
    "print(char_dic)\n",
    "\n",
    "dic_size = len(char_dic)\n",
    "print(f'문자 집합의 크기 : {dic_size}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 하이퍼 파라미터 설정\n",
    "hidden_size = dic_size\n",
    "sequence_length = 10 # 임의 숫자 가정\n",
    "learning_rate = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 if you wan -> f you want\n",
      "1 f you want ->  you want \n",
      "2  you want  -> you want t\n",
      "3 you want t -> ou want to\n",
      "4 ou want to -> u want to \n",
      "5 u want to  ->  want to b\n",
      "6  want to b -> want to bu\n",
      "7 want to bu -> ant to bui\n",
      "8 ant to bui -> nt to buil\n",
      "9 nt to buil -> t to build\n",
      "10 t to build ->  to build \n",
      "11  to build  -> to build a\n",
      "12 to build a -> o build a \n",
      "13 o build a  ->  build a s\n",
      "14  build a s -> build a sh\n",
      "15 build a sh -> uild a shi\n",
      "16 uild a shi -> ild a ship\n",
      "17 ild a ship -> ld a ship,\n",
      "18 ld a ship, -> d a ship, \n",
      "19 d a ship,  ->  a ship, d\n",
      "20  a ship, d -> a ship, do\n",
      "21 a ship, do ->  ship, don\n",
      "22  ship, don -> ship, don'\n",
      "23 ship, don' -> hip, don't\n",
      "24 hip, don't -> ip, don't \n",
      "25 ip, don't  -> p, don't d\n",
      "26 p, don't d -> , don't dr\n",
      "27 , don't dr ->  don't dru\n",
      "28  don't dru -> don't drum\n",
      "29 don't drum -> on't drum \n",
      "30 on't drum  -> n't drum u\n",
      "31 n't drum u -> 't drum up\n",
      "32 't drum up -> t drum up \n",
      "33 t drum up  ->  drum up p\n",
      "34  drum up p -> drum up pe\n",
      "35 drum up pe -> rum up peo\n",
      "36 rum up peo -> um up peop\n",
      "37 um up peop -> m up peopl\n",
      "38 m up peopl ->  up people\n",
      "39  up people -> up people \n",
      "40 up people  -> p people t\n",
      "41 p people t ->  people to\n",
      "42  people to -> people tog\n",
      "43 people tog -> eople toge\n",
      "44 eople toge -> ople toget\n",
      "45 ople toget -> ple togeth\n",
      "46 ple togeth -> le togethe\n",
      "47 le togethe -> e together\n",
      "48 e together ->  together \n",
      "49  together  -> together t\n",
      "50 together t -> ogether to\n",
      "51 ogether to -> gether to \n",
      "52 gether to  -> ether to c\n",
      "53 ether to c -> ther to co\n",
      "54 ther to co -> her to col\n",
      "55 her to col -> er to coll\n",
      "56 er to coll -> r to colle\n",
      "57 r to colle ->  to collec\n",
      "58  to collec -> to collect\n",
      "59 to collect -> o collect \n",
      "60 o collect  ->  collect w\n",
      "61  collect w -> collect wo\n",
      "62 collect wo -> ollect woo\n",
      "63 ollect woo -> llect wood\n",
      "64 llect wood -> lect wood \n",
      "65 lect wood  -> ect wood a\n",
      "66 ect wood a -> ct wood an\n",
      "67 ct wood an -> t wood and\n",
      "68 t wood and ->  wood and \n",
      "69  wood and  -> wood and d\n",
      "70 wood and d -> ood and do\n",
      "71 ood and do -> od and don\n",
      "72 od and don -> d and don'\n",
      "73 d and don' ->  and don't\n",
      "74  and don't -> and don't \n",
      "75 and don't  -> nd don't a\n",
      "76 nd don't a -> d don't as\n",
      "77 d don't as ->  don't ass\n",
      "78  don't ass -> don't assi\n",
      "79 don't assi -> on't assig\n",
      "80 on't assig -> n't assign\n",
      "81 n't assign -> 't assign \n",
      "82 't assign  -> t assign t\n",
      "83 t assign t ->  assign th\n",
      "84  assign th -> assign the\n",
      "85 assign the -> ssign them\n",
      "86 ssign them -> sign them \n",
      "87 sign them  -> ign them t\n",
      "88 ign them t -> gn them ta\n",
      "89 gn them ta -> n them tas\n",
      "90 n them tas ->  them task\n",
      "91  them task -> them tasks\n",
      "92 them tasks -> hem tasks \n",
      "93 hem tasks  -> em tasks a\n",
      "94 em tasks a -> m tasks an\n",
      "95 m tasks an ->  tasks and\n",
      "96  tasks and -> tasks and \n",
      "97 tasks and  -> asks and w\n",
      "98 asks and w -> sks and wo\n",
      "99 sks and wo -> ks and wor\n",
      "100 ks and wor -> s and work\n",
      "101 s and work ->  and work,\n",
      "102  and work, -> and work, \n",
      "103 and work,  -> nd work, b\n",
      "104 nd work, b -> d work, bu\n",
      "105 d work, bu ->  work, but\n",
      "106  work, but -> work, but \n",
      "107 work, but  -> ork, but r\n",
      "108 ork, but r -> rk, but ra\n",
      "109 rk, but ra -> k, but rat\n",
      "110 k, but rat -> , but rath\n",
      "111 , but rath ->  but rathe\n",
      "112  but rathe -> but rather\n",
      "113 but rather -> ut rather \n",
      "114 ut rather  -> t rather t\n",
      "115 t rather t ->  rather te\n",
      "116  rather te -> rather tea\n",
      "117 rather tea -> ather teac\n",
      "118 ather teac -> ther teach\n",
      "119 ther teach -> her teach \n",
      "120 her teach  -> er teach t\n",
      "121 er teach t -> r teach th\n",
      "122 r teach th ->  teach the\n",
      "123  teach the -> teach them\n",
      "124 teach them -> each them \n",
      "125 each them  -> ach them t\n",
      "126 ach them t -> ch them to\n",
      "127 ch them to -> h them to \n",
      "128 h them to  ->  them to l\n",
      "129  them to l -> them to lo\n",
      "130 them to lo -> hem to lon\n",
      "131 hem to lon -> em to long\n",
      "132 em to long -> m to long \n",
      "133 m to long  ->  to long f\n",
      "134  to long f -> to long fo\n",
      "135 to long fo -> o long for\n",
      "136 o long for ->  long for \n",
      "137  long for  -> long for t\n",
      "138 long for t -> ong for th\n",
      "139 ong for th -> ng for the\n",
      "140 ng for the -> g for the \n",
      "141 g for the  ->  for the e\n",
      "142  for the e -> for the en\n",
      "143 for the en -> or the end\n",
      "144 or the end -> r the endl\n",
      "145 r the endl ->  the endle\n",
      "146  the endle -> the endles\n",
      "147 the endles -> he endless\n",
      "148 he endless -> e endless \n",
      "149 e endless  ->  endless i\n",
      "150  endless i -> endless im\n",
      "151 endless im -> ndless imm\n",
      "152 ndless imm -> dless imme\n",
      "153 dless imme -> less immen\n",
      "154 less immen -> ess immens\n",
      "155 ess immens -> ss immensi\n",
      "156 ss immensi -> s immensit\n",
      "157 s immensit ->  immensity\n",
      "158  immensity -> immensity \n",
      "159 immensity  -> mmensity o\n",
      "160 mmensity o -> mensity of\n",
      "161 mensity of -> ensity of \n",
      "162 ensity of  -> nsity of t\n",
      "163 nsity of t -> sity of th\n",
      "164 sity of th -> ity of the\n",
      "165 ity of the -> ty of the \n",
      "166 ty of the  -> y of the s\n",
      "167 y of the s ->  of the se\n",
      "168  of the se -> of the sea\n",
      "169 of the sea -> f the sea.\n"
     ]
    }
   ],
   "source": [
    "# 임의의 sequence_length값 단위로 샘플들을 잘라서 데이터 만드는 모습\n",
    "\n",
    "# 데이터 구성\n",
    "x_data = []\n",
    "y_data = []\n",
    "\n",
    "for i in range(0, len(sentence) - sequence_length):\n",
    "    x_str = sentence[i:i + sequence_length]\n",
    "    y_str = sentence[i+1: i + sequence_length + 1]\n",
    "    print(i, x_str, '->', y_str)\n",
    "\n",
    "    x_data.append([char_dic[c] for c in x_str]) # x str to index\n",
    "    y_data.append([char_dic[c] for c in y_str]) # y str to index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11, 19, 21, 15, 2, 6, 21, 9, 10, 5]\n",
      "[19, 21, 15, 2, 6, 21, 9, 10, 5, 20]\n"
     ]
    }
   ],
   "source": [
    "print(x_data[0])\n",
    "print(y_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "훈련 데이터의 크기 : torch.Size([170, 10, 25])\n",
      "레이블의 크기 : torch.Size([170, 10])\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 1., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 1., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 1., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0.]])\n",
      "tensor([19, 21, 15,  2,  6, 21,  9, 10,  5, 20])\n"
     ]
    }
   ],
   "source": [
    "x_one_hot = [np.eye(dic_size)[x] for x in x_data] # x 데이터는 원-핫 인코딩\n",
    "X = torch.FloatTensor(x_one_hot)\n",
    "y = torch.LongTensor(y_data)\n",
    "\n",
    "print(f\"훈련 데이터의 크기 : {X.shape}\")\n",
    "print(f\"레이블의 크기 : {y.shape}\")\n",
    "print(X[0])\n",
    "print(y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 구현\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self, input_dim, hidden_size, layers): # 현재 hidden_size는 dic_size와 같음.\n",
    "        super(Net, self).__init__()\n",
    "        self.rnn = torch.nn.RNN(input_dim, hidden_size, num_layers=layers, batch_first = True)\n",
    "        self.fc = torch.nn.Linear(hidden_size, hidden_size, bias=True)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x, _status = self.rnn(x)\n",
    "        x = self.fc(x)\n",
    "        return x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([170, 10, 25])\n",
      "torch.Size([1700, 25])\n",
      "torch.Size([170, 10])\n",
      "torch.Size([1700])\n"
     ]
    }
   ],
   "source": [
    "net = Net(dic_size, hidden_size, 2) # 이번에는 층 2개\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(net.parameters(), learning_rate)\n",
    "\n",
    "outputs = net(X)\n",
    "print(outputs.shape) # 3차원 텐서\n",
    "print(outputs.view(-1, dic_size).shape) # 2차원 텐서로 변환\n",
    "print(y.shape)\n",
    "print(y.view(-1).shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lmmllmmllllllllllllllllldlmllllmlllllolllmllllmmmlllmllllomlmllmolllllllllllllllmmlllllmlmllllllllllmlmlllllllmllllllloldwomlllldlllllllmllmlmlwwlllllllllomllmlllllllllllmmlllllml\n",
      "       tp  eh  up   euepepp       e pe    ehp  e  u    e  eu  u e         up  h     eupppe  he  eup e ep  hh     up   ppe  eu   eehe  eu  u       ehe        ue      pp     ue  up \n",
      "ttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttttt\n",
      "       re                                                                                                                                                                          \n",
      "g..eo'ee'.h.'.......'....'.s........'.'le...g'....e.'l....e...eg.e..h.....l......e'.h..s'.......e..le'.s..'...s.l.'....l....'..e...e.'ee'.eg.e............l..sh.se'..e..f'.eh....e.\n",
      ".s eoeeoeeioeiiiiiwioeooiieieiioeeieeieioieoeiaioeaoeaoeiooioeoeeoioeooioeeoeioiiieoioeoeioiaoiieieioeeoioiiioioeoeoioioeoiaaoiweeoioeioeeoeeiioeooeioiiioeoeeooioeioiioioeioeioeoi\n",
      ".sdhoea eeooaoooooooneooeoeeeoeohoeaeohoooooeooe oenhooaa oonaoneoonooooohaoeoooooaoeoaohoooooooeoeooea eoooo ooeoeooooneoooooooeeoeooooaaonaoo eooaoooooneonaono ee eooeohoohe hon\n",
      "t bo ue ne  e eooe      e oeo e leeee   o  o  oo  e ao ee oo eoleoe  o e ee o  o  o e         o eee  ne    oe e e   eoe e  eo oole eo oe eole      e  o o eo t  o he ee e  o he ho \n",
      "t oo oe ne  t ee t      e eee e hteee   e  e  e   e ne te ee ne oee  e e et e  e  e t         e eee   o     e e oe  e e e  ee  e oeee ee ne n      e  e e e  oe    e ee e  e ne ne \n",
      "thoo        o                 t  t                t    o                  t                           o                                            t      e                        \n",
      "th o        t  e                 t                t    t                                                         e   h  o                 e                                        \n",
      "dh o  o     t oh t          o    t      o  o  o        o  h   o t         o    o  o t           e     t       t  h      o  o     t o      o        e      t  t        t    o       \n",
      "dhto t eo  to oh t       t po    o o   th      e  o t  or    t  o r       o    o  o t       t   e o   t    tr t t     t o      r o ot to to t r    o  o h o  t s     et o  o er e  \n",
      "dhto tto toto oo        to to   to o  oto  o t e to eo to to eo too    o oo t to  o e t i   o   eot s o   ttote eo    t o  oo so toeo to eo eo     o      tot  s   r et o to eo e  \n",
      "ihte  eo et t eostot  ootos tot toooo o oooooteo etoto eo to eo eo  oooo oeo  too toeo  i s oot toe isas   eo e eot  oo oo to tteeoeo to eo ttoo tootttt  eo iist tt si tott e  e  \n",
      "iiba  eo itto eostt thtst   aos ttott o ooto  eosteott e sto toteo  ooot teoi to stotossiss oot to istastseoo  stos  ot eo to  tetoeo eo to ttoo oooeo  d eo ii issist  tottoe  l  \n",
      "iita  to   ta thsst tut ts t oo t eo t tostos te tt e  e  to t too  oeo  oaoiito  t toitin  oe  th  stt   to tt tosttos    tooitlo eo to totttdor the en  to  itst stss toro eo l  \n",
      "i  o tto   to tois  oot tt tto  t eo s tos    to to e  l  to tore  toao  oaes too t tnssim  oe  to  stt   to t  ooistos    too  lo eo to to to  o toe  nd t  iih sis t ttort eo l  \n",
      "i lo et l  to to t  to it   tof t to t eostn  td to t  e  to to to tooo  ooos toot  thsim   oe  to n  t   to lh to    it   tont et e  to to to    toe  n  e    u     t  to toe     \n",
      "i  e  ton  to to t  to it   eof i ta   ao tl  to to tm t  to to to to on aons tor't ths m  toe  to t  t t tofk  tn  tos    to   etoe  to to to  t toe     eo  e t  t t  tf toe     \n",
      "i  eo tont tof'us   tetii   woy't taos to  eo te     t e  to to to  ooo  ooht too't thsst  eoe  to t  thd tor't tas  os    to   etoe  to to  to t tni     eo  end tt t  tf toe  e  \n",
      "t  eo to s ton u t  thsnk   won'thwans ths 'm  e    em e  to eo to  ooo  oahd ton't ahssi  toe  to ts ths aor t tao  es    do s eo e  to to  to t toer  d eosi ns t  t  to toe  n  \n",
      "t loo tons ton us   whtnts  won' hwans tosf ms epto  mhe  toaeo ees  aone ahs ton't aussin  oe  tost ithd aor   tnis es    do s eo e  torlo   ooa toe  nd essidns t  t  wo toe  n s\n",
      "i lor tont torbusts thtniss aon't aons tosh r  e  o  a e  toreorees  aon  asi aof't onisin toe  tonisitnt wor t tom  asner toas eo e  torbosntoor toer od essidns td t  wf aoeoeu s\n",
      "i lor tont todbose  thtnims tor't wont totf rd  st r rhe  to eo em t aon  anm tof'thwnikd  toe  tonksitnt wor d eus  asne  to c ea e  torbon't or toer nd essiins td t  wr aoe dh  \n",
      "i lo  tonk to luie  tntnim  ton't wont tot  a  e do  rhe  to lo te t won  ans ton't aniii  tee  tonksitnt oon   tum  as e  to c eahem to tend  or the end emsiin   d t  wf the de  \n",
      "i lo  tont torbose  tutnip  ton't wont boe  a  e doret e  te to te t fond and ton't asiii  teem tonksit t aonk  tum  as e  teac ethem to te d for the end emsiinm  d t  wf themeet \n",
      "i lo  tont to tose  thtndp  ton't wont loip o  e torem em to to te t fon' fnd ton't wsiii  teem tonks t d oonk, tut  at em teacheahem to ton' fo  themend emsiins  d t  wf toemeet \n",
      "i bo  tont to tosl  thtndp  ton't wont bo pem  e torem em terco le t wond ond ton't wssii  toem tosks ahd wonk, tut rat em teachethem to lon'tpor themend amsiinm  s th wf themeet \n",
      "inbor tont to butlt whthips ton't wont tn pem  e todethem terco lest wood wnd ton't assith toem tosks ahd wonk, tut rat em teach them to bon'tfor themend lmsiies  s th wf themeet \n",
      "inboretont to butlt whships ton't wout ao pem le todemhem terco lest wood wnd ton't assiih toem tosks ahd workt tuthrus em teach them to band for themend emsiiem rs th of themeet \n",
      "snboretont uo luilt tsships ton't aont bo pem le to erhem to to lo t word and don't assitn toem tosks asd work, tuthrushem toach them to lon' for themend essiiem asith of themeeti\n",
      "snboretont to lutld tnship, ton't aont uo per le to emher to lo le t wood fnd don't assiin toem tosks asd workt duthrashem toach ther to lon' for themend essiipm ns th of thereeti\n",
      "snboretont to lutld wnship, ton't aont uo per le to  mher te collest wood and don't assiin toem tosks and workt duthrashem toach ther to long for therendlessiimm ns ty of thereeti\n",
      "snyoretout to butld dsship, don't aout uo per le to ether te co lest wood and don't assitn toem tosks and work, buthrasher toach ther to bong for therendlessiimm ns ty of thereet \n",
      "mnyoretont to butld dnship, don't dout up per le to ether to co lect aood and don't assitn toem tosks and work, buthrashem toach ther to bong for therendlessiimm ns ty wf thereet \n",
      "mnyo etont to build dnship, don't aout up per le to emher to collect aood and don't assitn toem tosks and work, but rashem toach ther to bong for therendlessiimm ns ty wf thereet \n",
      "lnyo ethnt to build dnship, don't aout up pem le to ether to collect aood and don't assitn toem tosks and work, but rashem toach ther to bong for therendlessiimmens ty wf thereet \n",
      "lnyo ewont to build dnship, don't aoum up pem le to ether te collect wood and don't assign toem tosks and work, but rashem teach ther to bong for therendless immensity wf thereet \n",
      "ldlo ewont to build dsship, don't aoum up pem le together te collect wood and don't assign toem tosks and work, but rashem teach ther to bong for therendless immensity wf thereet \n",
      "mdlo  whnt to luild dsship, don't aoum up pemple together to collect wood and don't assign toem tosks and work, but rashem teach ther to long for therendless immensity of thereet \n",
      "mdlo  whnt to luild dhship, don't aoum up pepple together to collect wood and don't assign them tosks and work, but rathem teach ther to long for therendless immensity wf thereet \n",
      "mdlo  tont to luild dsship, don't arum up pepple together to collect wood and don't assign them tosks and work, but rathem teach them to long for therendless immensity wf the eet \n",
      "mnlo  whnt to luild asship, don't aoum up pepple together te collect wood and don't assign them tosks and work, but rathem teach ther to long for therendless immensity of the set \n",
      "mnlo  whnt to luild asship, don't arum up pepple together to collect wood and don't assign them tosks and work, but rather teach ther to long for therendless immensity of the set \n",
      "mnlo  wont to build asship, don't drum up people together to collect wood and don't dssign them tosks and work, but rather teach ther to bong for therendless immensity of the set \n",
      "mnlou wont to build asship, don't drum up people together to collect wood and don't dssign them tosks and aork, but rather teach ther to bong for therendless immensity of the set \n",
      "mnlou wont to build asship, don't drum up people together to collect wood and don't dssign them tosks and work, but rathem teach ther to bong for therendless immensity of the set \n",
      "mnlou wont to build asship, don't drum up people togethe  te collect wood and don't dssign them tosks and work, but rathem teach ther to bong for therendless immensity of the set \n",
      "mnyou wont to build asship, don't drum up people together te collect wood and don't dssign them tosks and work, but rathem teach them to bong for therendless immensity of the set \n",
      "mnyou want to build anship, don't drum up people together to collect wood and don't dssign them tosks and work, but rather toach them to bong for the endless immensity of the set \n",
      "mnyou want to build anship, don't arum up people together to collect wood and don't assign them tosks and work, but rather toach them to long for the endless immensity of the set \n",
      "mnyou want to build anship, don't arum up people together to collect wood and don't assign them tosks and work, but rather teach them to long for the endless immensity of the set \n",
      "mnyou want to build anship, don't drum up people together to collect wood and don't dssign them tosks and work, but rather teach them to long for the endless immensity of the set \n",
      "mnyou want to build asship, don't arum up people together to collect wood and don't assign them tosks and work, but rather teach them to long for the endless immensity of the set \n",
      "mnyou want to build asship, don't drum up people together to collect wood and don't assign them tosks and work, but rather teach them to long for the endless immensity of the set \n",
      "mnyou want to build a ship, don't arum up people together to collect wood and don't assign them tosks and work, but rather teach them to long for the endless immensity of the set \n",
      "mnyou want to build a ship, don't drum up people together to collect wood and don't dssign them tosks and work, but rather teach them to long for the endless immensity of the set \n",
      "mnyou want to build a ship, don't drum up people togethem to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the set \n",
      "gnyou want to build a ship, don't arum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the set \n",
      "gnyou want to build a ship, don't drum up people together to collect wood and don't dssign them tasks and work, but rather teach them to long for the endless immensity of the set \n",
      "gnyou want to build a ship, don't drum up people together to collect wood and don't dssign them tosks and work, but rather teach them to long for the endless immensity of the set \n",
      "gnyou want to build a ship, don't arum up people together to collect wood and don't assign them tasks and work, but rather teach them ta long for the endless immensity of the set \n",
      "gnyou want to build a ship, don't drum up people together to collect wood and don't dssign them tasks and work, but rather teach them ta long for the endless immensity of the set \n",
      "gnyou want to build a ship, don't drum up people together to collect wood and don't dssign them tasks and work, but rather teach them ta long for the endless immensity of the set \n",
      "g you want to build a ship, don't arum up people together to collect wood and don't assign them tosks and work, but rather teach them to long for the endless immensity of the set \n",
      "g you want to build a ship, don't arum up people together to collect wood and don't assign them tosks and work, but rather teach them to long for the endless immensity of the set \n",
      "g you want to build a ship, don't drum up people together to collect wood and don't dssign them tasks and work, but rather teach them ta long for the endless immensity of the sea \n",
      "p you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them ta long for the endless immensity of the sea \n",
      "g you want to build a ship, don't arum up people together to collect wood and don't assign them tasks and work, but rather teach them ta long for the endless immensity of the sea \n",
      "g you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea \n",
      "g you want to build a ship, don't arum up people together to collect wood and don't assign them tosks and work, but rather teach them to long for the endless immensity of the sea \n",
      "g you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea \n",
      "g you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them ta long for the endless immensity of the sea \n",
      "g you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them ta long for the endless immensity of the sea \n",
      "p you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them ta long for the endless immensity of the sea \n",
      "p you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them ta long for the endless immensity of the sea \n",
      "g you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them ta long for the endless immensity of the sea \n",
      "g you want to build a ship, don't drum up people together to collect wood and don't assign them tosks and work, but rather teach them to long for the endless immensity of the sea \n",
      "f you want to build a ship, don't drum up people together to collect wood and don't assign them tosks and work, but rather teach them to long for the endless immensity of the sea \n",
      "f you want to build a ship, don't drum up people together to collect wood and don't asnign them tosks and work, but rather teach them to long for the endless immensity of the sea \n",
      "f you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea \n",
      "f you want to build a ship, don't arum up people together to collect wood and don't assign them tasks and work, but rather teach them ta long for the endless immensity of the sea \n",
      "g you want to build a ship, don't drum up people together togcollect wood and don't dssign them tasks and work, but rather teach them ta long for the endless immensity of the sea \n",
      "g you want to build a ship, don't drum up people together to bollect wood and don't dssign them tasks and work, but rather toach them ta long for the endless immensity of the s a \n",
      "g you want to cuild a ship, don't arum up people together te collect wood and don't assign the  tosks and work, but rather teach them ta long for the endless immensity of the sea.\n",
      "g you want to build a ship, don't arum up people together to collect wood and don't assign them tosks and work, but rather te ch them to long for the endless iemensity of the eea.\n",
      "f you want to cuild a ship, don't arum up people together to collect wood and don't assign them tasks and work, but rather teach them ta long for the sndlessiimmensity of the seas\n",
      "f you want to build a ship, don't drum up oeople together togbollect wood and don't assign them tosks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "f you want to build a ship, don't drum up people together to bollect wood and don't dssign them tosks and wook, but rather teach them to long for the endleps immensity of the sea.\n",
      "f you want to build a ship, dnn't drum up people together to bollect wood and don't dssign them tosks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "f yo  want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "l you want to build a ship, don't arum up people together to collect wood and don't assign them tasks and work, but rather teach them ta long for the endless immensity of the sea.\n",
      "l you want to build a ship, don't arum up people together to collect wood and don't assign them tasks and work, but rather teach them ta long for the endless immensity of the sea.\n",
      "l you want to build a ship, don't drum up people together to collect wood and don't dssign them tasks and work, but rather teach them ta long for the endless immensity of the sea.\n",
      "l you want to build a ship, don't drum up people together to collect wood and don't dssign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "t you want to build a ship, don't arum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "g you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "p you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "p you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n"
     ]
    }
   ],
   "source": [
    "for i in range(100):\n",
    "    optimizer.zero_grad()\n",
    "    outputs = net(X) # (170, 10, 25) 크기를 가진 텐서를 매 에포크 마다 모델의 입력으로 사용\n",
    "    loss = criterion(outputs.view(-1, dic_size), y.view(-1))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # results의 텐서 크기는 (170, 10)\n",
    "    results = outputs.argmax(dim=2)\n",
    "    predict_str = \"\"\n",
    "    for j, result in enumerate(results):\n",
    "        if j == 0 : # 처음에는 예측 결과를 전부 가져오지만\n",
    "            predict_str += ''.join([char_set[t] for t in result])\n",
    "        else : # 그 다음에는 마지막 글자만 반복 추가\n",
    "            predict_str += char_set[result[-1]]\n",
    "    print(predict_str)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
