{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 07. [DL 입문] - 순환 신경망(Recurrent Neural Network)\n",
    "\n",
    "cell : hidden layer에서 activation function을 통해 결과를 내보내는 역할\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "# 파이썬으로 RNN 구현\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "timesteps = 10  # 시점의 수. NLP 에서 보통 문장의 길이가 된다.\n",
    "input_size = 4  # 입력의 차원. NLP 에서 보통 단어 벡터의 차원이 된다.\n",
    "hidden_size = 8  # 은닉 상태의 크기. 메모리 셀의 용량이다.\n",
    "\n",
    "inputs = np.random.random((timesteps, input_size))  # 입력에 해당되는 2D 텐서\n",
    "\n",
    "hidden_state_t = np.zeros((hidden_size,))  # 초기 은닉 상태는 0 vector로 초기화\n",
    "# 은닉 상태의 크기 hidden_size 로 은닉 상태를 만듬.\n",
    "\n",
    "print(hidden_state_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8, 4)\n",
      "(8, 8)\n",
      "(8,)\n"
     ]
    }
   ],
   "source": [
    "Wx = np.random.random(\n",
    "    (hidden_size, input_size)\n",
    ")  # (8,4) 크기의 2D 텐서 생성, 입력에 대한 가중치.\n",
    "Wh = np.random.random(\n",
    "    (hidden_size, hidden_size)\n",
    ")  # (8,8) 크기의 2D 텐서 생성, 은닉 상태에 대한 가중치.\n",
    "b = np.random.random((hidden_size,))  # (8,) 크기의 1D 텐서 생성. 이 값은 편향(bias)\n",
    "\n",
    "print(np.shape(Wx))\n",
    "print(np.shape(Wh))\n",
    "print(np.shape(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 8)\n",
      "(2, 8)\n",
      "(3, 8)\n",
      "(4, 8)\n",
      "(5, 8)\n",
      "(6, 8)\n",
      "(7, 8)\n",
      "(8, 8)\n",
      "(9, 8)\n",
      "(10, 8)\n",
      "[[0.9692762  0.76549866 0.85653569 0.91474923 0.87628104 0.85454369\n",
      "  0.96317985 0.96867712]\n",
      " [0.99999351 0.99993606 0.99984555 0.99996551 0.99997311 0.99997925\n",
      "  0.99998283 0.99999256]\n",
      " [0.99998908 0.99992825 0.99977554 0.99995359 0.99995399 0.99996073\n",
      "  0.99996233 0.99995808]\n",
      " [0.99999049 0.999936   0.99960676 0.99983949 0.99997566 0.99997925\n",
      "  0.99995508 0.99994487]\n",
      " [0.99999615 0.99996168 0.99989102 0.99998725 0.99996519 0.99997823\n",
      "  0.99998415 0.9999921 ]\n",
      " [0.99998902 0.99993365 0.99973035 0.99989782 0.99996905 0.99997254\n",
      "  0.9999581  0.99993299]\n",
      " [0.99999278 0.9999495  0.9997939  0.99993438 0.99997498 0.99997986\n",
      "  0.99997124 0.99996553]\n",
      " [0.99999081 0.99992728 0.99983186 0.99995483 0.99990484 0.99994562\n",
      "  0.99994381 0.99992649]\n",
      " [0.99999383 0.99995494 0.99989093 0.99997369 0.99996493 0.99997467\n",
      "  0.99997764 0.99997264]\n",
      " [0.99999484 0.99995601 0.99981177 0.99996352 0.99997532 0.99998188\n",
      "  0.99997846 0.99998545]]\n"
     ]
    }
   ],
   "source": [
    "total_hidden_states = []\n",
    "\n",
    "# 메모리 셀 동작\n",
    "for input_t in inputs:  # 각 시점에 따라서 입력값이 입력됨.\n",
    "    output_t = np.tanh(np.dot(Wx, input_t) + np.dot(Wh, hidden_state_t) + b)\n",
    "    total_hidden_states.append(\n",
    "        list(output_t)\n",
    "    )  # 각 시점의 은닉 상태의 값을 계속해서 축적\n",
    "    print(\n",
    "        np.shape(total_hidden_states)\n",
    "    )  # 각 시점 t별 메모리 셀의 출력의 크기는 (timestep, output_dim)\n",
    "    hidden_state_t = output_t\n",
    "\n",
    "total_hidden_states = np.stack(total_hidden_states, axis=0)\n",
    "# 출력시 값을 깔끔하게 해준다.\n",
    "\n",
    "print(\n",
    "    total_hidden_states\n",
    ")  # (timesteps, output_dim)의 크기. 이 경우 (10,8)의 크기를 가지는 메모리 셀의 2D 텐서를 출력."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파이토치의 nn.RNN()\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 10, 8])\n",
      "torch.Size([1, 1, 8])\n"
     ]
    }
   ],
   "source": [
    "input_size = 5  # 입력의 크기 -> 매 시점마다 들어가는 입력의 크기\n",
    "hidden_size = 8  # 은닉 상태의 크기\n",
    "\n",
    "# 입력 텐서\n",
    "# (batch_size, time_steps, input_size)\n",
    "inputs = torch.Tensor(1, 10, 5)\n",
    "\n",
    "cell = nn.RNN(\n",
    "    input_size, hidden_size, batch_first=True\n",
    ")  # 입력 텐서의 첫번째 차원이 배치 크기\n",
    "\n",
    "outputs, _status = cell(\n",
    "    inputs\n",
    ")  # outputs : 모든 시점의 은닉 상태, _status : 마지막 시점 은닉 상태\n",
    "\n",
    "print(outputs.shape)  # 모든 time-step 의 hidden_state\n",
    "print(_status.shape)  # 최종 time-step의 hidden_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 10, 8])\n",
      "torch.Size([2, 1, 8])\n"
     ]
    }
   ],
   "source": [
    "# Deep RNN\n",
    "\n",
    "# (batch_size, time_steps, input_size)\n",
    "inputs = torch.Tensor(1, 10, 5)\n",
    "\n",
    "cell = nn.RNN(input_size=5, hidden_size=8, num_layers=2, batch_first=True)\n",
    "outputs, _status = cell(inputs)\n",
    "\n",
    "print(outputs.shape)\n",
    "print(_status.shape)  # (층의 갯수, 배치 크기, 은닉 상태의 크기)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 10, 16])\n",
      "torch.Size([4, 1, 8])\n"
     ]
    }
   ],
   "source": [
    "# Bidirectional RNN\n",
    "\n",
    "# (batch_size, time_steps, input_size)\n",
    "inputs = torch.Tensor(1, 10, 5)\n",
    "\n",
    "cell = nn.RNN(\n",
    "    input_size=5, hidden_size=8, num_layers=2, batch_first=True, bidirectional=True\n",
    ")\n",
    "outputs, _status = cell(inputs)\n",
    "\n",
    "print(outputs.shape)  # 배치 크기, 시퀀스 길이, 은닉 상태의 크기 x 2\n",
    "print(_status.shape)  # (층의 개수 x2, 배치 크기, 은닉 상태의 크기)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM과 GRU\n",
    "- 바닐라 RNN의 한계 : time-step이 길어질수록 앞의 정보가 뒤로 충분히 전달되지 못함\n",
    "- 장기 의존성 문제(the problem of Long-Term Dependencies)\n",
    "\n",
    "### LSTM\n",
    "- Long SHort-Term Memory\n",
    "1. 입력 게이트\n",
    "    - 현재의 정보를 기억하기 위한 게이트\n",
    "2. 삭제 게이트\n",
    "    - 기억을 삭제하기 위한 게이트\n",
    "3. 셀 상태(장기 상태)\n",
    "    - 입력 게이트와 삭제 게이트의 조합\n",
    "4. 출력 게이트와 은닉 상태(단기 상태)\n",
    "    \n",
    "### GRU\n",
    "1. 업데이트 게이트\n",
    "2. 리셋 게이트\n",
    "\n",
    "LSTM, GRU 성능은 비슷하다고 알려짐\n",
    "경험적으로 데이터 양이 ㅈ거을 때는 매개변수의 양이 적은 GRU가 조금 낫고\n",
    "데이터 양이 많으면 LSTM이 더 낫다고도 함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파이토치의 nn.LSTM()\n",
    "# nn.LSTM(input_dim, hidden_size, batch_first = True)\n",
    "\n",
    "# 파이토치의 nn.GRU()\n",
    "# nn.GRU(input_dim, hidden_size, batch_first = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문자 단위 RNN (Char RNN)\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "문자 집합의 크기 : 5\n"
     ]
    }
   ],
   "source": [
    "# 1.훈련 데이터 전처리하기\n",
    "# apple -> pple!\n",
    "\n",
    "input_str = \"apple\"\n",
    "label_str = \"pple!\"\n",
    "char_vocab = sorted(list(set(input_str + label_str)))\n",
    "vocab_size = len(char_vocab)\n",
    "\n",
    "print(f\"문자 집합의 크기 : {vocab_size}\")  # !,a,e,l,p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = vocab_size  # 입력의 크기는 문자 집합의 크기, 원핫벡터사용\n",
    "hidden_size = 5\n",
    "output_size = 5\n",
    "learning_rate = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'!': 0, 'a': 1, 'e': 2, 'l': 3, 'p': 4}\n"
     ]
    }
   ],
   "source": [
    "char_to_index = dict((c,i) for i,c in enumerate(char_vocab)) # 문자에 고유한 정수 인덱스 부여\n",
    "print(char_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: '!', 1: 'a', 2: 'e', 3: 'l', 4: 'p'}\n"
     ]
    }
   ],
   "source": [
    "# 예측 결과를 다시 문자 시퀀스로 보기 위해 정수로부터 문자를 얻을 수 있는 index_to_char\n",
    "index_to_char = {}\n",
    "for key, value in char_to_index.items():\n",
    "    index_to_char[value] = key\n",
    "print(index_to_char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 4, 4, 3, 2]\n",
      "[4, 4, 3, 2, 0]\n"
     ]
    }
   ],
   "source": [
    "# 입력 데이터와 레이블 데이터의 각 문자들을 정수로 맵핑\n",
    "x_data = [char_to_index[c] for c in input_str]\n",
    "y_data = [char_to_index[c] for c in label_str]\n",
    "print(x_data)\n",
    "print(y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 4, 4, 3, 2]]\n",
      "[[4, 4, 3, 2, 0]]\n"
     ]
    }
   ],
   "source": [
    "# 파이토치 nn.RNN()은 기본적으로 3차원 텐서 받음\n",
    "# 배치 차원 추가\n",
    "# 텐서 연산인 unsqueeze(0)을 통해 해결할 수도 있었음.\n",
    "x_data = [x_data]\n",
    "y_data =[y_data]\n",
    "print(x_data)\n",
    "print(y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[0., 1., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 1.],\n",
      "       [0., 0., 0., 0., 1.],\n",
      "       [0., 0., 0., 1., 0.],\n",
      "       [0., 0., 1., 0., 0.]])]\n"
     ]
    }
   ],
   "source": [
    "# 입력 시퀀스의 각 문자들을 원-핫 벡터로 바꿔줌\n",
    "x_one_hot = [np.eye(vocab_size)[x] for x in x_data]\n",
    "print(x_one_hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "훈련 데이터의 크기 : torch.Size([1, 5, 5])\n",
      "레이블의 크기 : torch.Size([1, 5])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/m5/8hy8yjzn7kj7v2vfxdvj1j7r0000gn/T/ipykernel_32268/1266846071.py:2: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_new.cpp:257.)\n",
      "  X = torch.FloatTensor(x_one_hot)\n"
     ]
    }
   ],
   "source": [
    "# 입력 데이터와 레이블 데이터를 텐서로 바꿔줌\n",
    "X = torch.FloatTensor(x_one_hot)\n",
    "y = torch.LongTensor(y_data)\n",
    "\n",
    "# 각 텐서 크기 확인\n",
    "print(f'훈련 데이터의 크기 : {X.shape}')\n",
    "print(f'레이블의 크기 : {y.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 5, 5])\n",
      "torch.Size([5, 5])\n",
      "torch.Size([1, 5])\n",
      "torch.Size([5])\n"
     ]
    }
   ],
   "source": [
    "# 모델 구현\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(Net, self).__init__()\n",
    "        self.rnn = torch.nn.RNN(input_size, hidden_size, batch_first = True) # RNN 셀 구현\n",
    "        self.fc = torch.nn.Linear(hidden_size, output_size, bias=True) # 출력층 구현\n",
    "    \n",
    "    def forward(self, x): # 구현한 RNN 셀과 출력층을 연결\n",
    "        x, _status = self.rnn(x)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "    \n",
    "net = Net(input_size, hidden_size, output_size)\n",
    "\n",
    "outputs = net(X)\n",
    "\n",
    "print(outputs.shape) # 3차원 텐서\n",
    "print(outputs.view(-1, input_size).shape) # 2차원 텐서로 변환\n",
    "print(y.shape)\n",
    "print(y.view(-1).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 loss:  1.7478086948394775 prediction:  [[1 1 1 1 1]] true y:  [[4, 4, 3, 2, 0]] prediction str:  aaaaa\n",
      "1 loss:  1.4563896656036377 prediction:  [[4 4 4 4 3]] true y:  [[4, 4, 3, 2, 0]] prediction str:  ppppl\n",
      "2 loss:  1.3306190967559814 prediction:  [[4 4 4 4 4]] true y:  [[4, 4, 3, 2, 0]] prediction str:  ppppp\n",
      "3 loss:  1.2274348735809326 prediction:  [[4 4 4 4 0]] true y:  [[4, 4, 3, 2, 0]] prediction str:  pppp!\n",
      "4 loss:  1.1237967014312744 prediction:  [[4 4 4 4 0]] true y:  [[4, 4, 3, 2, 0]] prediction str:  pppp!\n",
      "5 loss:  1.0067049264907837 prediction:  [[4 4 4 4 0]] true y:  [[4, 4, 3, 2, 0]] prediction str:  pppp!\n",
      "6 loss:  0.8778664469718933 prediction:  [[4 4 4 2 0]] true y:  [[4, 4, 3, 2, 0]] prediction str:  pppe!\n",
      "7 loss:  0.732934296131134 prediction:  [[4 4 3 2 0]] true y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "8 loss:  0.5749741792678833 prediction:  [[4 4 3 2 0]] true y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "9 loss:  0.45975661277770996 prediction:  [[4 4 4 2 0]] true y:  [[4, 4, 3, 2, 0]] prediction str:  pppe!\n",
      "10 loss:  0.32330334186553955 prediction:  [[4 4 3 2 0]] true y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "11 loss:  0.23629966378211975 prediction:  [[4 4 3 2 0]] true y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "12 loss:  0.16593867540359497 prediction:  [[4 4 3 2 0]] true y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "13 loss:  0.10745485126972198 prediction:  [[4 4 3 2 0]] true y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "14 loss:  0.07354768365621567 prediction:  [[4 4 3 2 0]] true y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "15 loss:  0.05579831451177597 prediction:  [[4 4 3 2 0]] true y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "16 loss:  0.03865345939993858 prediction:  [[4 4 3 2 0]] true y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "17 loss:  0.024662422016263008 prediction:  [[4 4 3 2 0]] true y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "18 loss:  0.01661290042102337 prediction:  [[4 4 3 2 0]] true y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "19 loss:  0.012106157839298248 prediction:  [[4 4 3 2 0]] true y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "20 loss:  0.009313310496509075 prediction:  [[4 4 3 2 0]] true y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "21 loss:  0.007404158357530832 prediction:  [[4 4 3 2 0]] true y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "22 loss:  0.006010913755744696 prediction:  [[4 4 3 2 0]] true y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "23 loss:  0.00495568010956049 prediction:  [[4 4 3 2 0]] true y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "24 loss:  0.00414109043776989 prediction:  [[4 4 3 2 0]] true y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "25 loss:  0.0035058893263339996 prediction:  [[4 4 3 2 0]] true y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "26 loss:  0.003007271094247699 prediction:  [[4 4 3 2 0]] true y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "27 loss:  0.0026130678597837687 prediction:  [[4 4 3 2 0]] true y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "28 loss:  0.0022989187855273485 prediction:  [[4 4 3 2 0]] true y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "29 loss:  0.002046293579041958 prediction:  [[4 4 3 2 0]] true y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "30 loss:  0.0018410931807011366 prediction:  [[4 4 3 2 0]] true y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "31 loss:  0.0016725482419133186 prediction:  [[4 4 3 2 0]] true y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "32 loss:  0.0015327774453908205 prediction:  [[4 4 3 2 0]] true y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "33 loss:  0.001415652222931385 prediction:  [[4 4 3 2 0]] true y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "34 loss:  0.0013164191041141748 prediction:  [[4 4 3 2 0]] true y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "35 loss:  0.0012316785287111998 prediction:  [[4 4 3 2 0]] true y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "36 loss:  0.001158671686425805 prediction:  [[4 4 3 2 0]] true y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "37 loss:  0.001095305196940899 prediction:  [[4 4 3 2 0]] true y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "38 loss:  0.0010397231671959162 prediction:  [[4 4 3 2 0]] true y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "39 loss:  0.0009908785577863455 prediction:  [[4 4 3 2 0]] true y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "40 loss:  0.0009475100669078529 prediction:  [[4 4 3 2 0]] true y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "41 loss:  0.0009088322403840721 prediction:  [[4 4 3 2 0]] true y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "42 loss:  0.0008742022328078747 prediction:  [[4 4 3 2 0]] true y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "43 loss:  0.0008430012385360897 prediction:  [[4 4 3 2 0]] true y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "44 loss:  0.0008147768676280975 prediction:  [[4 4 3 2 0]] true y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "45 loss:  0.0007890768465586007 prediction:  [[4 4 3 2 0]] true y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "46 loss:  0.0007657105452381074 prediction:  [[4 4 3 2 0]] true y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "47 loss:  0.0007442256319336593 prediction:  [[4 4 3 2 0]] true y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "48 loss:  0.0007245982997119427 prediction:  [[4 4 3 2 0]] true y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "49 loss:  0.0007064237142913043 prediction:  [[4 4 3 2 0]] true y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "50 loss:  0.0006896066479384899 prediction:  [[4 4 3 2 0]] true y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "51 loss:  0.0006740281241945922 prediction:  [[4 4 3 2 0]] true y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "52 loss:  0.0006595927407033741 prediction:  [[4 4 3 2 0]] true y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "53 loss:  0.0006460623699240386 prediction:  [[4 4 3 2 0]] true y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "54 loss:  0.0006334609934128821 prediction:  [[4 4 3 2 0]] true y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "55 loss:  0.0006216692854650319 prediction:  [[4 4 3 2 0]] true y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "56 loss:  0.0006105445791035891 prediction:  [[4 4 3 2 0]] true y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "57 loss:  0.0006001343717798591 prediction:  [[4 4 3 2 0]] true y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "58 loss:  0.0005902481498196721 prediction:  [[4 4 3 2 0]] true y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "59 loss:  0.0005809811991639435 prediction:  [[4 4 3 2 0]] true y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "60 loss:  0.0005722144851461053 prediction:  [[4 4 3 2 0]] true y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "61 loss:  0.0005638526636175811 prediction:  [[4 4 3 2 0]] true y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "62 loss:  0.0005559195997193456 prediction:  [[4 4 3 2 0]] true y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "63 loss:  0.0005484391003847122 prediction:  [[4 4 3 2 0]] true y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "64 loss:  0.0005412444588728249 prediction:  [[4 4 3 2 0]] true y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "65 loss:  0.0005344308447092772 prediction:  [[4 4 3 2 0]] true y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "66 loss:  0.000527926895301789 prediction:  [[4 4 3 2 0]] true y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "67 loss:  0.0005216849967837334 prediction:  [[4 4 3 2 0]] true y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "68 loss:  0.000515657477080822 prediction:  [[4 4 3 2 0]] true y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "69 loss:  0.0005098919500596821 prediction:  [[4 4 3 2 0]] true y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "70 loss:  0.0005043646087870002 prediction:  [[4 4 3 2 0]] true y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "71 loss:  0.000499075511470437 prediction:  [[4 4 3 2 0]] true y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "72 loss:  0.0004939769278280437 prediction:  [[4 4 3 2 0]] true y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "73 loss:  0.000489068974275142 prediction:  [[4 4 3 2 0]] true y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "74 loss:  0.00048428005538880825 prediction:  [[4 4 3 2 0]] true y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "75 loss:  0.00047968170838430524 prediction:  [[4 4 3 2 0]] true y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "76 loss:  0.00047525009722448885 prediction:  [[4 4 3 2 0]] true y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "77 loss:  0.00047096144407987595 prediction:  [[4 4 3 2 0]] true y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "78 loss:  0.0004667441826313734 prediction:  [[4 4 3 2 0]] true y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "79 loss:  0.0004627413582056761 prediction:  [[4 4 3 2 0]] true y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "80 loss:  0.0004587861767504364 prediction:  [[4 4 3 2 0]] true y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "81 loss:  0.0004549977311398834 prediction:  [[4 4 3 2 0]] true y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "82 loss:  0.00045130454236641526 prediction:  [[4 4 3 2 0]] true y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "83 loss:  0.0004476589965634048 prediction:  [[4 4 3 2 0]] true y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "84 loss:  0.00044410876580514014 prediction:  [[4 4 3 2 0]] true y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "85 loss:  0.0004406775697134435 prediction:  [[4 4 3 2 0]] true y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "86 loss:  0.0004373655538074672 prediction:  [[4 4 3 2 0]] true y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "87 loss:  0.0004341249878052622 prediction:  [[4 4 3 2 0]] true y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "88 loss:  0.0004309082287363708 prediction:  [[4 4 3 2 0]] true y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "89 loss:  0.0004277867265045643 prediction:  [[4 4 3 2 0]] true y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "90 loss:  0.00042473673238418996 prediction:  [[4 4 3 2 0]] true y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "91 loss:  0.00042182966717518866 prediction:  [[4 4 3 2 0]] true y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "92 loss:  0.00041882722871378064 prediction:  [[4 4 3 2 0]] true y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "93 loss:  0.00041596783557906747 prediction:  [[4 4 3 2 0]] true y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "94 loss:  0.00041320367017760873 prediction:  [[4 4 3 2 0]] true y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "95 loss:  0.00041043950477615 prediction:  [[4 4 3 2 0]] true y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "96 loss:  0.00040777065441943705 prediction:  [[4 4 3 2 0]] true y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "97 loss:  0.0004051017458550632 prediction:  [[4 4 3 2 0]] true y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "98 loss:  0.0004025758244097233 prediction:  [[4 4 3 2 0]] true y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n",
      "99 loss:  0.00040002609603106976 prediction:  [[4 4 3 2 0]] true y:  [[4, 4, 3, 2, 0]] prediction str:  pple!\n"
     ]
    }
   ],
   "source": [
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(net.parameters(), learning_rate)\n",
    "\n",
    "for i in range(100):\n",
    "    optimizer.zero_grad()\n",
    "    outputs = net(X)\n",
    "    loss = criterion(outputs.view(-1, input_size), y.view(-1)) # view 하는 이유는 batch 차원 제거 위해\n",
    "    loss.backward() # 기울기 계산\n",
    "    optimizer.step()\n",
    "\n",
    "    # 모델이 실제로 어떻게 예측했는지를 확인하기 위한 코드\n",
    "    result = outputs.data.numpy().argmax(axis=2) # 최종 예측값인 각 time-step 별 5차원 벡터에 대해서 가장 높은 값의 인덱스를 선택\n",
    "    result_str = ''.join([index_to_char[c] for c in np.squeeze(result)])\n",
    "    print(i, \"loss: \", loss.item(), \"prediction: \", result, \"true y: \", y_data, \"prediction str: \", result_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'f': 0, 'h': 1, 'd': 2, 'a': 3, 'i': 4, 'b': 5, 'u': 6, 't': 7, \"'\": 8, ',': 9, 'm': 10, 'w': 11, 'y': 12, ' ': 13, 'k': 14, 'p': 15, 'c': 16, 'o': 17, 'r': 18, '.': 19, 'n': 20, 'e': 21, 's': 22, 'g': 23, 'l': 24}\n",
      "문자 집합의 크기 : 25\n"
     ]
    }
   ],
   "source": [
    "# 더 많은 데이터로 학습한 문자 단위 RNN\n",
    "\n",
    "# 훈련 데이터 전처리\n",
    "\n",
    "sentence = (\"if you want to build a ship, don't drum up people together to \"\n",
    "            \"collect wood and don't assign them tasks and work, but rather \"\n",
    "            \"teach them to long for the endless immensity of the sea.\")\n",
    "\n",
    "char_set = list(set(sentence)) # 중복 제거한 문자 집합 생성\n",
    "char_dic = {c : i for i, c in enumerate(char_set)} # 각 문자에 정수 인코딩\n",
    "print(char_dic)\n",
    "\n",
    "dic_size = len(char_dic)\n",
    "print(f'문자 집합의 크기 : {dic_size}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 하이퍼 파라미터 설정\n",
    "hidden_size = dic_size\n",
    "sequence_length = 10 # 임의 숫자 가정\n",
    "learning_rate = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 if you wan -> f you want\n",
      "1 f you want ->  you want \n",
      "2  you want  -> you want t\n",
      "3 you want t -> ou want to\n",
      "4 ou want to -> u want to \n",
      "5 u want to  ->  want to b\n",
      "6  want to b -> want to bu\n",
      "7 want to bu -> ant to bui\n",
      "8 ant to bui -> nt to buil\n",
      "9 nt to buil -> t to build\n",
      "10 t to build ->  to build \n",
      "11  to build  -> to build a\n",
      "12 to build a -> o build a \n",
      "13 o build a  ->  build a s\n",
      "14  build a s -> build a sh\n",
      "15 build a sh -> uild a shi\n",
      "16 uild a shi -> ild a ship\n",
      "17 ild a ship -> ld a ship,\n",
      "18 ld a ship, -> d a ship, \n",
      "19 d a ship,  ->  a ship, d\n",
      "20  a ship, d -> a ship, do\n",
      "21 a ship, do ->  ship, don\n",
      "22  ship, don -> ship, don'\n",
      "23 ship, don' -> hip, don't\n",
      "24 hip, don't -> ip, don't \n",
      "25 ip, don't  -> p, don't d\n",
      "26 p, don't d -> , don't dr\n",
      "27 , don't dr ->  don't dru\n",
      "28  don't dru -> don't drum\n",
      "29 don't drum -> on't drum \n",
      "30 on't drum  -> n't drum u\n",
      "31 n't drum u -> 't drum up\n",
      "32 't drum up -> t drum up \n",
      "33 t drum up  ->  drum up p\n",
      "34  drum up p -> drum up pe\n",
      "35 drum up pe -> rum up peo\n",
      "36 rum up peo -> um up peop\n",
      "37 um up peop -> m up peopl\n",
      "38 m up peopl ->  up people\n",
      "39  up people -> up people \n",
      "40 up people  -> p people t\n",
      "41 p people t ->  people to\n",
      "42  people to -> people tog\n",
      "43 people tog -> eople toge\n",
      "44 eople toge -> ople toget\n",
      "45 ople toget -> ple togeth\n",
      "46 ple togeth -> le togethe\n",
      "47 le togethe -> e together\n",
      "48 e together ->  together \n",
      "49  together  -> together t\n",
      "50 together t -> ogether to\n",
      "51 ogether to -> gether to \n",
      "52 gether to  -> ether to c\n",
      "53 ether to c -> ther to co\n",
      "54 ther to co -> her to col\n",
      "55 her to col -> er to coll\n",
      "56 er to coll -> r to colle\n",
      "57 r to colle ->  to collec\n",
      "58  to collec -> to collect\n",
      "59 to collect -> o collect \n",
      "60 o collect  ->  collect w\n",
      "61  collect w -> collect wo\n",
      "62 collect wo -> ollect woo\n",
      "63 ollect woo -> llect wood\n",
      "64 llect wood -> lect wood \n",
      "65 lect wood  -> ect wood a\n",
      "66 ect wood a -> ct wood an\n",
      "67 ct wood an -> t wood and\n",
      "68 t wood and ->  wood and \n",
      "69  wood and  -> wood and d\n",
      "70 wood and d -> ood and do\n",
      "71 ood and do -> od and don\n",
      "72 od and don -> d and don'\n",
      "73 d and don' ->  and don't\n",
      "74  and don't -> and don't \n",
      "75 and don't  -> nd don't a\n",
      "76 nd don't a -> d don't as\n",
      "77 d don't as ->  don't ass\n",
      "78  don't ass -> don't assi\n",
      "79 don't assi -> on't assig\n",
      "80 on't assig -> n't assign\n",
      "81 n't assign -> 't assign \n",
      "82 't assign  -> t assign t\n",
      "83 t assign t ->  assign th\n",
      "84  assign th -> assign the\n",
      "85 assign the -> ssign them\n",
      "86 ssign them -> sign them \n",
      "87 sign them  -> ign them t\n",
      "88 ign them t -> gn them ta\n",
      "89 gn them ta -> n them tas\n",
      "90 n them tas ->  them task\n",
      "91  them task -> them tasks\n",
      "92 them tasks -> hem tasks \n",
      "93 hem tasks  -> em tasks a\n",
      "94 em tasks a -> m tasks an\n",
      "95 m tasks an ->  tasks and\n",
      "96  tasks and -> tasks and \n",
      "97 tasks and  -> asks and w\n",
      "98 asks and w -> sks and wo\n",
      "99 sks and wo -> ks and wor\n",
      "100 ks and wor -> s and work\n",
      "101 s and work ->  and work,\n",
      "102  and work, -> and work, \n",
      "103 and work,  -> nd work, b\n",
      "104 nd work, b -> d work, bu\n",
      "105 d work, bu ->  work, but\n",
      "106  work, but -> work, but \n",
      "107 work, but  -> ork, but r\n",
      "108 ork, but r -> rk, but ra\n",
      "109 rk, but ra -> k, but rat\n",
      "110 k, but rat -> , but rath\n",
      "111 , but rath ->  but rathe\n",
      "112  but rathe -> but rather\n",
      "113 but rather -> ut rather \n",
      "114 ut rather  -> t rather t\n",
      "115 t rather t ->  rather te\n",
      "116  rather te -> rather tea\n",
      "117 rather tea -> ather teac\n",
      "118 ather teac -> ther teach\n",
      "119 ther teach -> her teach \n",
      "120 her teach  -> er teach t\n",
      "121 er teach t -> r teach th\n",
      "122 r teach th ->  teach the\n",
      "123  teach the -> teach them\n",
      "124 teach them -> each them \n",
      "125 each them  -> ach them t\n",
      "126 ach them t -> ch them to\n",
      "127 ch them to -> h them to \n",
      "128 h them to  ->  them to l\n",
      "129  them to l -> them to lo\n",
      "130 them to lo -> hem to lon\n",
      "131 hem to lon -> em to long\n",
      "132 em to long -> m to long \n",
      "133 m to long  ->  to long f\n",
      "134  to long f -> to long fo\n",
      "135 to long fo -> o long for\n",
      "136 o long for ->  long for \n",
      "137  long for  -> long for t\n",
      "138 long for t -> ong for th\n",
      "139 ong for th -> ng for the\n",
      "140 ng for the -> g for the \n",
      "141 g for the  ->  for the e\n",
      "142  for the e -> for the en\n",
      "143 for the en -> or the end\n",
      "144 or the end -> r the endl\n",
      "145 r the endl ->  the endle\n",
      "146  the endle -> the endles\n",
      "147 the endles -> he endless\n",
      "148 he endless -> e endless \n",
      "149 e endless  ->  endless i\n",
      "150  endless i -> endless im\n",
      "151 endless im -> ndless imm\n",
      "152 ndless imm -> dless imme\n",
      "153 dless imme -> less immen\n",
      "154 less immen -> ess immens\n",
      "155 ess immens -> ss immensi\n",
      "156 ss immensi -> s immensit\n",
      "157 s immensit ->  immensity\n",
      "158  immensity -> immensity \n",
      "159 immensity  -> mmensity o\n",
      "160 mmensity o -> mensity of\n",
      "161 mensity of -> ensity of \n",
      "162 ensity of  -> nsity of t\n",
      "163 nsity of t -> sity of th\n",
      "164 sity of th -> ity of the\n",
      "165 ity of the -> ty of the \n",
      "166 ty of the  -> y of the s\n",
      "167 y of the s ->  of the se\n",
      "168  of the se -> of the sea\n",
      "169 of the sea -> f the sea.\n"
     ]
    }
   ],
   "source": [
    "# 임의의 sequence_length값 단위로 샘플들을 잘라서 데이터 만드는 모습\n",
    "\n",
    "# 데이터 구성\n",
    "x_data = []\n",
    "y_data = []\n",
    "\n",
    "for i in range(0, len(sentence) - sequence_length):\n",
    "    x_str = sentence[i:i + sequence_length]\n",
    "    y_str = sentence[i+1: i + sequence_length + 1]\n",
    "    print(i, x_str, '->', y_str)\n",
    "\n",
    "    x_data.append([char_dic[c] for c in x_str]) # x str to index\n",
    "    y_data.append([char_dic[c] for c in y_str]) # y str to index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4, 0, 13, 12, 17, 6, 13, 11, 3, 20]\n",
      "[0, 13, 12, 17, 6, 13, 11, 3, 20, 7]\n"
     ]
    }
   ],
   "source": [
    "print(x_data[0])\n",
    "print(y_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "훈련 데이터의 크기 : torch.Size([170, 10, 25])\n",
      "레이블의 크기 : torch.Size([170, 10])\n",
      "tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
      "         0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 1., 0., 0., 0., 0.]])\n",
      "tensor([ 0, 13, 12, 17,  6, 13, 11,  3, 20,  7])\n"
     ]
    }
   ],
   "source": [
    "x_one_hot = [np.eye(dic_size)[x] for x in x_data] # x 데이터는 원-핫 인코딩\n",
    "X = torch.FloatTensor(x_one_hot)\n",
    "y = torch.LongTensor(y_data)\n",
    "\n",
    "print(f\"훈련 데이터의 크기 : {X.shape}\")\n",
    "print(f\"레이블의 크기 : {y.shape}\")\n",
    "print(X[0])\n",
    "print(y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 구현\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self, input_dim, hidden_size, layers): # 현재 hidden_size는 dic_size와 같음.\n",
    "        super(Net, self).__init__()\n",
    "        self.rnn = torch.nn.RNN(input_dim, hidden_size, num_layers=layers, batch_first = True)\n",
    "        self.fc = torch.nn.Linear(hidden_size, hidden_size, bias=True)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x, _status = self.rnn(x)\n",
    "        x = self.fc(x)\n",
    "        return x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([170, 10, 25])\n",
      "torch.Size([1700, 25])\n",
      "torch.Size([170, 10])\n",
      "torch.Size([1700])\n"
     ]
    }
   ],
   "source": [
    "net = Net(dic_size, hidden_size, 2) # 이번에는 층 2개\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(net.parameters(), learning_rate)\n",
    "\n",
    "outputs = net(X)\n",
    "print(outputs.shape) # 3차원 텐서\n",
    "print(outputs.view(-1, dic_size).shape) # 2차원 텐서로 변환\n",
    "print(y.shape)\n",
    "print(y.view(-1).shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nrrrrrrrrlrrrrrrrnrrrrrnrnnrrrrlnrrrrnrrrnrrrrnrrrrrlnrrllnrrryrrrrlrrrrrnrrlrrrrnnrrrnnrlrrrrlrrrrnrnrrnrrrrlnrrrrrrlnrrllnrrrlrrrrlrrrrrrrnlrrrrnrrlrrlrrrnnrrrrrlnrrrrrrrrrlrnrr\n",
      "   nnnnnnn  onnnnn  nnnnnnn nnnnn   nnn n nnnnn nno nnnn nno nnn  nn  nnn nnn  nnnn  nnn nnn nnnn nnn nnn  nn nnnnn   nn  nno nn   nnnn nnnnnn nn non nnnn nnn nnnnnnn  nnnnonnnnnn\n",
      "                                                                                                                                                                                   \n",
      "smbm.m.m.m.m.mm''.bkmk.m.m.m.mbm.m.mambb.m.m.kbbkbbbm.m.m.b.'m.mb'bmkbm'm''k''.mnm.m.mnm.m.m.mkkbb.kkm.m.m.m.m.m.m.m.m.m.m.'.kkk'b..msm.'b'm.'bm'm'm.mb'.m.m.m.b.m.bk.b.mskb.bmbbbm\n",
      "nrwoaodmni.isiiibahsmi.iaiainiaiaiainmsiaisiiniaiioiini.iam.iioi.oami.i.i..oilbiainiaismsiaili.swaoosiwiaosoaiaiaisinininiaisiabiwoamm.ahosi.oaibob.aininiamsiwoaisib.aoib..aaosi.i\n",
      "eothdstohhet hhohhhotoehhhhoeheoehuhet hhhotohahhthtoe ehhtohheohehtohoeheeheotoehthehtt hhhoeheehhitoehhttheoeoehtoeheoeiuaohhhohhhtothhitohehtohahhheoehhttheohho ehhhhaehhhitoeh\n",
      "eothctt hchtpccthcw t ipihwhtoi ehwhpt hcwttpcihcchthchwhct ichticht c hhwihi thipthcpttthcw cpehhcct ihwtthihehwpt whihwrcm ccc ihct hhuct hihttcthipitwhw thhhcctiihhcchwhhctt wp\n",
      "o tecet i htpweticw t e eoeht e ioe e tmmetttwmoew epcpehctticw wmot meemeemw eheothw t tmpe e chhm t ihe toiheoe t eoehe e trce mmm  ehmct imhetrthi w eoe tpemw tteehrcoeemmot ep\n",
      "  ter t d o  e  toc e d e eht e i'e e  e e o toeoe e e'ehr  e e eo   e eettor e e toe t  m   e etom t dhe tod e e t ioehe e  r e eoo  tom e em'o   he e i e t'e e  tteercedeoeot e \n",
      "   er t d o  e  t r   t e e t e t e o  t e   d e e e e e    e e d    e e tt e e e t e    e   r etoe   dod tod e d t t d e e  r   toe  toe   de      d d d e   d e   teee edt e   e \n",
      "   er   e       t e   dod d t d t e    t d     e  od o e    e o d    t a  t     e   e    e     otoe   dod tod d d t t d d        to   toe   ds      d d d o   t e   ts   oet e   e \n",
      " d    t         t r tod e d t r t a   ht      he t e  he  t eao ee   tr   d  ht   t e    s she  toe   a e to  d d t d   e  t       e  to     sa   t e   d o   t    ts      t       \n",
      " t    h    h    t e tot d t tor t e   hto   o he toer he  t e   ee thto h to ht e t e t t    e  to  t t e on hd d t d she  t   t toe  to  o   a r the t t e t t    ht      t e t e \n",
      " t r  to t er c t d dot d dotod t doechth teo we t d  he  toec d e t to   tod t t t eot tettoer toe t thd eo  t d t d d e  t e t t e  t ele ttt r the t t e tht   slth er ot e th  \n",
      " t e  tert eoec t d dot d dododot t ec t  toocwe thdethe  toec d eot to   dod eodpt t tpie t e  t e t thd eo  t t t d t ep t ect t e  t ece  tt r t e tothe tit   e si er ot e th  \n",
      " t r  tort eoec t e d t d d d d t doec t  ls chert ertherct ecep ect dor  dod todpt d t ie t em torst t d doe t d t d t er toect t e storcep ip r t ert t d sii t e si erc t e t t \n",
      " i r  tors goec t e d t d tor d t aoet t  to cwert ers er toec i emt dor  d d dodpt d siiest eostoest t d dor t d t dot er t ect t e storcem ip r t emt t e sii s ussi erc t eosh  \n",
      " ilo mtors aonc t e a t d t s d t donm t  toncwe t ems er tonc i emt aor  d d dos't a si est eo tonst t d dou t d t dot er t ect t eo tonlem ip r t eot i e sii n nsti ers t eothi \n",
      " dlo ttors donc i e t t d d d t t donp a   onshemt emt er toncep o t dor  d d tod t aotiie s e  torst t d don t d t d t e  tonct the  tonceptit r t e t d e sii n nsst ors toeot n \n",
      " tloo tort dooc ild d t dpthd d t donm te  ooptemt  m her toncople t doo  t d thn't ds i s t e  tonct t d dooct d i n t e  to ch them to lom  p r t emt d e t tsn nm t out theot r \n",
      " tlooctorthdooc ild d t dpthn n t do m ts poophemto m her to copee t aooc t d the't as i t t er toect tsd tooct dhton t em to ch them to lom  por thert d e t tln nmsiyert themth  \n",
      " tlo  torthdoocoile t t dp,hd s t doum tsepeo he toemther to copae t dooc t d toe't dmt is l e  toe t dnd doo t dns e t e  to ch them to co dntor thert d e t iln n  ioect doe t gs\n",
      " tlooctort donceild d t ildhd d t douc yp paophe t emther to cople t do c t d thd't dssigp ther togpt dsd tor t dnt dst er to chetheo to lom  por the t d e t ign n siyoot themt gp\n",
      " dlo ctort wo cuild d t iltod d t druc bpuplophe t  o her to cople t dooc tndltod't dnsig sther togpt dsd dorc, dnt dst er toechethec to lom  lor the t d e t igne csi  od thert g \n",
      " dlo ctort woo'ui d d thild d d'tharu  upspoople t eether to colle t aooc tnd don'thastiig the  toe t dnd doo , dnt e t er toechethe  to lo dnfor the t d e t ign  piia op toe t gs\n",
      " dlooctost won'uildrd thip, d d't aouk upuleople t get er to cople t wooc tnd don't w'siigsther toeks dndldorc, dnt ast er toach ther to loe  por t e t d e t imnencst oop the t gp\n",
      " dloadlost wouc i d a t ip, don't woup up elople toge  er toncople t torc tnd don't wnsignst er to ks dnd dorct dnd ast er toach ther to lomp por t e t d e t imuencst oop the t gp\n",
      "kdlorctost aoocui n anthip, don's aru  up poople'thges e  to cople t aood and don's ast gnsthe  toaks dnd dorct dns a t e  toach the  to lon  p r t e t d e t im e pityoop the t gm\n",
      "kdlorctost to child anthip, don't aouk up penshe togms em to cople t tood and aon't assiin them toeks dnd aorct dnt   t em toach the  to lormepor the t d e s im e pit oop the t gm\n",
      "l  orctost do cuildra thipt don's aru  up penphe together to coplect tood and aon't astiinkthem toeks tsd aorct dut  st er toach ther to con  f r themt d e tiim e sity of themt  m\n",
      "l  orchosd do build a thip, don't douk upllensle togeu er to colle t tord and don't dssig  them toeks dsd dorc, dut a t em to ch ther to lonu for the t d e t im  ussty of the t mm\n",
      "l  or tost do build a thip, don't dru  up people th ether to collect tood wnd don't dssignsther tosks dnd dorc, dut a t er toach ther to lonl por the t d e s im  usity of thert is\n",
      "l  or tost wo build a thip, don't dru  up people sogether to collect wood wnd aon't dssign ther tosks dnd dorc, dut r t er toach the  to lon npor the s d e s im   sity of the s im\n",
      "l  onptost wo cuild a thip, don't dru  up peop e th ether to collect wood tnd aon't dssig  ther tosks dnd aorc, dut wat er toach the  toglong por the s d e s imte sity of the s gs\n",
      "l  onptont wo cuild a thip, don't druk up penple th ether to collect tood tnd aon't dssignsther tosks dnd aorc, dut r t er toach the  to long por the e d ess imte sity of the t ms\n",
      "k  o ptost wo cuild a ship, don't drum up penple to ether to collect tood tnd aon't dssign them tosks dnd aorc, dut rat er toach the  toglong permthere d ess imme sity of the t ms\n",
      "k  o ptost to cuild a ship, don't drum up penple torether to collect tord tnd aon't dssig  them tosks dnd aork, dut rrt er toach them togcong formthe e d ess im e sity of the t mm\n",
      "  ro  tost to cuild a ship, don't drum up people to ether to collect tood and aon't dssignether tosks and aorc, dut rather toach ther to long pormthere d ess imme sity of the e mm\n",
      "   o  tost wo cuild a ship, don't drum up people together to collect tood and aon't dssignethem tosks and aork, dut rather toach them to long for themtnd ess immensity of the s ms\n",
      "  pon tont to cuild a ship, don't drum up peoplemtogether to collect tood and aon't dssign them tosks and aork, dut wather toach ther to long for themtnd ess immeusity of themt ms\n",
      "  pon tont to cuild a ship, don't drum up people thgether to collect wood and aon't dssign them tosks and aork, dut wather toach ther to long for themend ess immensity of the t ms\n",
      "  pon woet to cuild a ship, don't drum up people together to collect wood wnd aon't dssign them tosks and aork, dut rat er toach them to long for the end ess immensity of toe e ms\n",
      "  pon wont to cuild a ship, don't drum up people thgether to collect wood wnd aon't dssign the  tosks wnd aork, dut rather toach them to long for the end ess immensity of the e ms\n",
      "  pon wont to cuild a ship, don't drum up people together to collect wood wnd aon't dssign them tosks wnd aork, dut rather toach them to long for the snd ess immensity of the s ms\n",
      "  pon wont to cuild a ship, don't arum up people together to collect aood wnd don't dssign them tosks and dork, dut rather toach them to long for the end ess immensity of theme as\n",
      "  yon wont to cuild a ship, don't arum up people together to collect aood wnd don't dssign them tosks and dork, but rather toach them to long for themend ess immensity of themt as\n",
      "  yon want to cuild a ship, don't drum up people together to collect aood wnd aon't dssign them tosks and aork, but rather toach them to long for themendless immensity of thems as\n",
      "  yon want to cuild a ship, don't arum up people together to collect aood wnd aon't assign them tosks and aork, but rather toach them to long for themendless immensity of the s am\n",
      "  you want to build a ship, don't arum up people together to collect aood wnd aon't dssign them tasks and aork, but rather toach them to long for themendless immensity of the s mm\n",
      "l you want to build a ship, don't drum up people together to collect aood wnd don't dssign them tasks and aork, but rather toach them ta long for the endless immensity of the s mm\n",
      "l you want to build a ship, don't drum up people together to collect aood wnd don't dssign them tasks and aork, but rather toach them ta long for the endless immensity of the s ms\n",
      "  you want to build a ship, don't drum up people together to collect aood wnd aon't dssign them tasks and work, but rather toach them to long for the endless immensity of the s ms\n",
      "f you want to build a ship, don't drum up people together to collect aood and don't dssign them tasks and work, but rather toach them to long for the endless immensity of the s am\n",
      "f you want to build a ship, don't drum up people together to collect wood and don't dssign them tasks and work, but rather toach them ta long for the endless immensity of the seas\n",
      "f you want to build a ship, don't drum up people together to collect wood and don't dssign them tasks and work, but rather toach them to long for the endless immensity of the seas\n",
      "f you want to build a ship, don't drum up people together to collect wood and don't dssign them tasks and work, but rather toach them to long for the endless immensity of the seam\n",
      "f you want to build a ship, don't drum up people together to collect wood and don't dssign them tosks and work, but rather toach them to long for the endless immensity of the seam\n",
      "f you want to build a ship, don't drum up people together to collect wood and don't dssign them tasks and work, but rather toach them to long for the endless immensity of the seas\n",
      "f you want to build a ship, don't drum up people together to collect wood and don't dssign them tasks and work, but rather toach them to long for the endless immensity of the seam\n",
      "f you want to build a ship, don't drum up people together to collect wood and don't dssign them tasks and work, but rather toach them to long for the endless immensity of the seam\n",
      "g you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather toach them to long for the endless immensity of the seas\n",
      "p you want to build a ship, don't drum up people together to collect wood and don't dssign them tasks and work, but rather toach them to long for the endless immensity of the seas\n",
      "p you want to build a ship, don't drum up people together to collect wood and don't dssign them tasks and work, but rather teach them to long for the endless immensity of the seac\n",
      "p you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the seak\n",
      "g you want to build a ship, don't drum up people together to collect wood and don't dssign them tasks and work, but rather teach them to long for the endless immensity of the seas\n",
      "f you want to build a ship, don't drum up people together to collect wood and don't dssign them tasks and work, but rather teach them to long for the endless immensity of the seac\n",
      "f you want to build a ship, don't drum up people together to collect wood and don't dssign them tasks and work, but rather toach them to long for the endless immensity of the seak\n",
      "g you want to build a ship, don't drum up people together to collect wood and don't dssign them tasks and work, but rather toach them to long for the endless immensity of the seas\n",
      "g you want to build a ship, don't drum up people together to collect wood and don't dssign them tasks and work, but rather toach them to long for the endless immensity of the seas\n",
      "g you want to build a ship, don't drum up people together to collect wood and don't dssign them tasks and work, but rather teach them to long for the endless immensity of the seak\n",
      "g you want to build a ship, don't drum up people together to collect wood and don't dssign them tasks and work, but rather teach them to long for the endless immensity of the seak\n",
      "f you want to build a ship, don't drum up people together to collect wood and don't dssign them tasks and work, but rather teach them to long for the endless immensity of the seak\n",
      "f you want to build a ship, don't drum up people together to collect wood and don't dssign them tasks and work, but rather teach them to long for the endless immensity of the seak\n",
      "g you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the seak\n",
      "g you want to build a ship, don't drum up people together to collect wood and don't dssign them tasks and work, but rather teach them to long for the endless immensity of the seak\n",
      "m you want to build a ship, don't drum up people together to collect wood and don't dssign them tasks and work, but rather teach them to long for the endless immensity of the seak\n",
      "m you want to build a ship, don't drum up people together to collect wood and don't dssign them tasks and work, but rather teach them to long for the endless immensity of the seas\n",
      "m you want to build a ship, don't drum up people together to collect wood and don't dssign them tasks and work, but rather teach them to long for the endless immensity of the seak\n",
      "m you want to build a ship, don't drum up people together to collect wood and don't dssign them tasks and work, but rather teach them to long for the endless immensity of the seak\n",
      "p you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the seak\n",
      "p you want to build a ship, don't drum up people together to collect wood and don't dssign them tasks and work, but rather teach them to long for the endless immensity of the seak\n",
      "p you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "m you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the seak\n",
      "l you want to build a ship, don't drum up people together to collect wood and don't dssign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "m you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the seak\n",
      "m you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the seak\n",
      "g you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "l you want to build a ship, don't drum up people together to collect wood and don't dssign them tasks and work, but rather teach them to long for the endless immensity of the seak\n",
      "m you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the sndless immensity of the seak\n",
      "m you want to build a ship, don't drum up people together to collect wood and don't dssign them tasks and work, but rather teach them to long for the endless immensity of the eea.\n",
      "g you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "g you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "l you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "f you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "m you want to build a ship, don't drum up people together to collect wood and don't dssign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "m you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "m you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "l you want to build a ship, don't drum up people together to collect wood and don't dssign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "l you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n",
      "m you want to build a ship, don't drum up people together to collect wood and don't assign them tasks and work, but rather teach them to long for the endless immensity of the sea.\n"
     ]
    }
   ],
   "source": [
    "for i in range(100):\n",
    "    optimizer.zero_grad()\n",
    "    outputs = net(X) # (170, 10, 25) 크기를 가진 텐서를 매 에포크 마다 모델의 입력으로 사용\n",
    "    loss = criterion(outputs.view(-1, dic_size), y.view(-1))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # results의 텐서 크기는 (170, 10)\n",
    "    results = outputs.argmax(dim=2)\n",
    "    predict_str = \"\"\n",
    "    for j, result in enumerate(results):\n",
    "        if j == 0 : # 처음에는 예측 결과를 전부 가져오지만\n",
    "            predict_str += ''.join([char_set[t] for t in result])\n",
    "        else : # 그 다음에는 마지막 글자만 반복 추가\n",
    "            predict_str += char_set[result[-1]]\n",
    "    print(predict_str)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 08. CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN으로 MNIST 분류하기\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "텐서의 크기 : torch.Size([1, 1, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "# 배치크기 x 채널 x 높이(height) x 너비(width)의 크기의 텐서를 선언\n",
    "inputs = torch.Tensor(1,1,28,28)\n",
    "print(f'텐서의 크기 : {inputs.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n"
     ]
    }
   ],
   "source": [
    "# 1번째 합성곱층 \n",
    "# 1채널 짜리를 입력받아서 32채널을 뽑아내는데 커널 사이즈는 3 패딩은 1\n",
    "conv1 = nn.Conv2d(1, 32, 3, padding=1)\n",
    "print(conv1)\n",
    "\n",
    "# 2번째 합성곱층\n",
    "conv2 = nn.Conv2d(32,64, kernel_size = 3, padding = 1)\n",
    "print(conv2)\n",
    "\n",
    "# max pooling\n",
    "pool = nn.MaxPool2d(2)\n",
    "print(pool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 32, 28, 28])\n",
      "torch.Size([1, 32, 14, 14])\n",
      "torch.Size([1, 64, 14, 14])\n",
      "torch.Size([1, 64, 7, 7])\n"
     ]
    }
   ],
   "source": [
    "# 구현체 연결하기\n",
    "out = conv1(inputs)\n",
    "print(out.shape)\n",
    "out = pool(out)\n",
    "print(out.shape)\n",
    "out = conv2(out)\n",
    "print(out.shape)\n",
    "out = pool(out)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3136])\n"
     ]
    }
   ],
   "source": [
    "# 첫번째 차원인 배치 차원은 그대로 두고 나머지는 펼쳐라\n",
    "out = out.view(out.size(0), -1)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 10])\n"
     ]
    }
   ],
   "source": [
    "fc = nn.Linear(3136, 10) # input_dim = 3136, output_dim = 10\n",
    "out = fc(out)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN으로 MNIST 분류\n",
    "import torchvision.datasets as dsets\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1148df4f0>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "torch.manual_seed(777)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "training_epochs = 15\n",
    "batch_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_train = dsets.MNIST(root='MNIST_data/', train=True, transform=transforms.ToTensor(), download=True)\n",
    "mnist_test = dsets.MNIST(root='MNIST_data/', train=False, transform = transforms.ToTensor(), download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = torch.utils.data.DataLoader(dataset=mnist_train, batch_size = batch_size, shuffle=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        # 첫번째층\n",
    "        # ImgIn shape = (?, 28, 28, 1)\n",
    "        # Conv -> (?, 28, 28, 32)\n",
    "        # Pool -> (?, 14, 14, 32)\n",
    "        self.layer1 = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "\n",
    "        # 두번쨰 층\n",
    "        # ImgIn shape= (?, 14, 14, 32)\n",
    "        # Conv -> (?, 14, 14, 64)\n",
    "        # Pool -> (?, 7, 7, 64)\n",
    "        self.layer2 = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "\n",
    "        # 전결합층 7x7x64 inputs -> 10 outputs\n",
    "        self.fc = torch.nn.Linear( 7 * 7 * 64, 10, bias=True)\n",
    "\n",
    "        # 전결합층 한정으로 가중치 초기화\n",
    "        torch.nn.init.xavier_uniform_(self.fc.weight)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = out.view(out.size(0), -1) # Flatten\n",
    "        out = self.fc(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CNN().to(device)\n",
    "criterion = torch.nn.CrossEntropyLoss().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "총 배치의 수 : 600\n"
     ]
    }
   ],
   "source": [
    "total_batch = len(data_loader)\n",
    "print(f'총 배치의 수 : {total_batch}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch:    1 cost = 0.224776238]\n",
      "[Epoch:    2 cost = 0.0623787791]\n",
      "[Epoch:    3 cost = 0.0460401401]\n",
      "[Epoch:    4 cost = 0.0373748913]\n",
      "[Epoch:    5 cost = 0.0311053004]\n",
      "[Epoch:    6 cost = 0.0260943454]\n",
      "[Epoch:    7 cost = 0.0215723086]\n",
      "[Epoch:    8 cost = 0.0179226175]\n",
      "[Epoch:    9 cost = 0.016017681]\n",
      "[Epoch:   10 cost = 0.013170178]\n",
      "[Epoch:   11 cost = 0.0102514522]\n",
      "[Epoch:   12 cost = 0.00959763303]\n",
      "[Epoch:   13 cost = 0.00850353297]\n",
      "[Epoch:   14 cost = 0.00647186674]\n",
      "[Epoch:   15 cost = 0.00649533421]\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(training_epochs):\n",
    "    avg_cost = 0 # 에포크당 평균 비용을 저장하기 위한 변수 초기화\n",
    "\n",
    "    for X,Y in data_loader: # 미니 배치 단위로 꺼내옴. \n",
    "        X = X.to(device)\n",
    "        Y = Y.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        hypothesis = model(X)\n",
    "        cost = criterion(hypothesis, Y)\n",
    "        cost.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        avg_cost += cost / total_batch # 현재 배치의 비용을 전체 배치수로 나누어 누적\n",
    "        \n",
    "    print(f'[Epoch: {epoch +1:>4} cost = {avg_cost:>.9}]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/solkim/Desktop/projects/my_python_project/DL_pytorch/.venv/lib/python3.11/site-packages/torchvision/datasets/mnist.py:81: UserWarning: test_data has been renamed data\n",
      "  warnings.warn(\"test_data has been renamed data\")\n",
      "/Users/solkim/Desktop/projects/my_python_project/DL_pytorch/.venv/lib/python3.11/site-packages/torchvision/datasets/mnist.py:71: UserWarning: test_labels has been renamed targets\n",
      "  warnings.warn(\"test_labels has been renamed targets\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9861999750137329\n"
     ]
    }
   ],
   "source": [
    "# 테스트\n",
    "with torch.no_grad():\n",
    "    # 테스트 데이터를 모델에 입력하기 위한 준비\n",
    "    X_test = mnist_test.test_data.view(len(mnist_test),1,28,28).float().to(device) # 텍스트 데이터의 크기를 맞추고, 연산을 위한 장치로 이동\n",
    "    Y_test = mnist_test.test_labels.to(device)\n",
    "\n",
    "    prediction = model(X_test)\n",
    "    correct_prediction = torch.argmax(prediction, 1) == Y_test\n",
    "    accuracy = correct_prediction.float().mean()\n",
    "    print('Accuracy:', accuracy.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    }
   ],
   "source": [
    "# deep CNN 층 5개\n",
    "import torchvision.datasets as dsets\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.init\n",
    "\n",
    "device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "torch.manual_seed(777)\n",
    "\n",
    "learning_rate = 0.001\n",
    "training_epochs = 15\n",
    "batch_size = 100\n",
    "\n",
    "mnist_train = dsets.MNIST(root='MNIST_data/', train=True, transform=transforms.ToTensor(), download=True)\n",
    "mnist_test = dsets.MNIST(root='MNIST_data/', train=False, transform = transforms.ToTensor(), download=True)\n",
    "\n",
    "data_loader = torch.utils.data.DataLoader(dataset=mnist_train, batch_size = batch_size, shuffle=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.keep_prob = 0.5 # 드롭아웃 확률\n",
    "        \n",
    "        self.layer1 = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(1,32,3,padding=1),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.MaxPool2d(2)\n",
    "        )\n",
    "\n",
    "        self.layer2 = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(32,64,3,padding=1),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.MaxPool2d(2)\n",
    "        )\n",
    "\n",
    "        self.layer3 = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(64,128,3,padding=1),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.MaxPool2d(kernel_size=2, stride=2, padding=1)\n",
    "        )\n",
    "\n",
    "        self.fc1 = torch.nn.Linear(4*4*128, 625, bias=True)\n",
    "        torch.nn.init.xavier_uniform_(self.fc1.weight)\n",
    "\n",
    "        self.layer4 = torch.nn.Sequential(\n",
    "            self.fc1,\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Dropout(p=1-self.keep_prob)\n",
    "        )\n",
    "\n",
    "        self.fc2 = torch.nn.Linear(625,10, bias=True)\n",
    "        torch.nn.init.xavier_uniform_(self.fc2.weight)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.layer4(out)\n",
    "        out = self.fc2(out)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "총 배치의 수 : 600\n"
     ]
    }
   ],
   "source": [
    "model = CNN().to(device)\n",
    "criterion = torch.nn.CrossEntropyLoss().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)\n",
    "\n",
    "total_batch = len(data_loader)\n",
    "print(f'총 배치의 수 : {total_batch}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch:    1] cost = 0.188369885\n",
      "[Epoch:    2] cost = 0.050810352\n",
      "[Epoch:    3] cost = 0.0379315503\n",
      "[Epoch:    4] cost = 0.0289165415\n",
      "[Epoch:    5] cost = 0.0238476582\n",
      "[Epoch:    6] cost = 0.0195887685\n",
      "[Epoch:    7] cost = 0.0179885831\n",
      "[Epoch:    8] cost = 0.0150393946\n",
      "[Epoch:    9] cost = 0.0121504636\n",
      "[Epoch:   10] cost = 0.0119604692\n",
      "[Epoch:   11] cost = 0.0108334674\n",
      "[Epoch:   12] cost = 0.00912533235\n",
      "[Epoch:   13] cost = 0.00868707709\n",
      "[Epoch:   14] cost = 0.00856188685\n",
      "[Epoch:   15] cost = 0.00764752226\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(training_epochs):\n",
    "    avg_cost = 0\n",
    "\n",
    "    for X, Y in data_loader:\n",
    "        X = X.to(device)\n",
    "        Y = Y.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        hypothesis = model(X)\n",
    "        cost = criterion(hypothesis, Y)\n",
    "        cost.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        avg_cost += cost / total_batch\n",
    "    \n",
    "    print( f'[Epoch: {epoch +1:>4}] cost = {avg_cost:>.9}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/solkim/Desktop/projects/my_python_project/DL_pytorch/.venv/lib/python3.11/site-packages/torchvision/datasets/mnist.py:81: UserWarning: test_data has been renamed data\n",
      "  warnings.warn(\"test_data has been renamed data\")\n",
      "/Users/solkim/Desktop/projects/my_python_project/DL_pytorch/.venv/lib/python3.11/site-packages/torchvision/datasets/mnist.py:71: UserWarning: test_labels has been renamed targets\n",
      "  warnings.warn(\"test_labels has been renamed targets\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9793000221252441\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    X_test = mnist_test.test_data.view(len(mnist_test), 1,28,28).float().to(device)\n",
    "    Y_test = mnist_test.test_labels.to(device)\n",
    "    prediction = model(X_test)\n",
    "    correct_prediction = torch.argmax(prediction, 1) == Y_test\n",
    "    accuracy = correct_prediction.float().mean()\n",
    "    print('Accuracy:', accuracy.item())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
