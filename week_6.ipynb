{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 09 [NLP 입문] -NLP Basics\n",
    "### 09-01 Tokenization\n",
    "#### 1. Word Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 토큰화 중 생기는 선택의 순간\n",
    "# Don't be fooled by the dark sounding name, Mr. Jone's Orphanage is as cheery as cheery goes for a pastry shop.\n",
    "# Don't , Jone's 토큰화 하는 방법 다양\n",
    "\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "from tensorflow.keras.preprocessing.text import text_to_word_sequence\n",
    "import nltk\n",
    "\n",
    "nltk.download(\"punkt_tab\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word_tokenize\n",
    "print(\n",
    "    \"단어 토큰화1 :\",\n",
    "    word_tokenize(\n",
    "        \"Don't be fooled by the dark sounding name, Mr. Jone's Orphanage is as cheery as cheery goes for a pastry shop.\"\n",
    "    ),\n",
    ")\n",
    "#'Do', \"n't\", 'Jone', \"'s\","
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    \"단어 토큰화2 :\",\n",
    "    WordPunctTokenizer().tokenize(\n",
    "        \"Don't be fooled by the dark sounding name, Mr. Jone's Orphanage is as cheery as cheery goes for a pastry shop.\"\n",
    "    ),\n",
    ")\n",
    "#'Don', \"'\", 't','Jone', \"'\", 's'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    \"단어 토큰화3 :\",\n",
    "    text_to_word_sequence(\n",
    "        \"Don't be fooled by the dark sounding name, Mr. Jone's Orphanage is as cheery as cheery goes for a pastry shop.\"\n",
    "    ),\n",
    ")\n",
    "# \"don't\", \"jone's\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 토큰화에서 고려해야할 사항\n",
    "1. 구두점, 특수문자 단순 제외해서는 안됨\n",
    "    1. 마침표(.)도 문장의 경계를 알 수 있음\n",
    "    2. 단어 자체에 구두점 있는 경우도 있음 Ph.D, $45.55\n",
    "2. 줄임말\n",
    "    1. we're -> re는 접어(clitic)\n",
    "3. 단어 내에 띄어쓰기가 있는 경우\n",
    "    1. New York 하나의 단어\n",
    "    2. rock 'n' roll\n",
    "4. 표준 토큰화 예제(Penn Treebank Tokenization)\n",
    "    1. 하이푼으로 구성된 단어는 하나로 유지\n",
    "    2. doesn't 와 같이 아포스트로피로 '접어'가 함꼐하는 단어는 분리해준다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "\n",
    "tokenizer = TreebankWordTokenizer()\n",
    "\n",
    "text = \"Starting a home-based restaurant may be an ideal. it doesn't have a food chain or restaurant of their own.\"\n",
    "print(\"트리뱅크 워드토크나이저 :\", tokenizer.tokenize(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 문장 토큰화(Sentence Tokenization)\n",
    "1. Corpus 내에서 sentence 단위로 구분하는 작업\n",
    "2. Sentence segmentation 이라고도 함\n",
    "3. 마침표(.)는 문장 중간에 들어가는 경우 많다. ex) aaa@gmail.com으로 결과보내줘"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "text = \"His barber kept his word. But keeping such a huge secret to himself was driving him crazy. Finally, the barber went up a mountain and almost to the edge of a cliff. He dug a hole in the midst of some reeds. He looked about, to make sure no one was near.\"\n",
    "print(\"문장 토큰화1 :\", sent_tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"I am actively looking for Ph.D. students. and you are a Ph.D student.\"\n",
    "print(\"문장 토큰화2 :\", sent_tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install kss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KSS(Korean Sentence Splitter)\n",
    "import kss\n",
    "\n",
    "text = \"딥 러닝 자연어 처리가 재미있기는 합니다. 그런데 문제는 영어보다 한국어로 할 때 너무 어렵습니다. 이제 해보면 알걸요?\"\n",
    "print(\"한국어 문장 토큰화 :\", kss.split_sentences(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 한국어 토큰화의 어려움\n",
    "1. 영어는 New York 같은 합성어, he's 같은 줄임말 정도만 예외 처리후 띄어쓰기 기준으로 토큰화해도 단어 토큰화 잘 됨\n",
    "2. 한국어에서 띄어쓰기 단위는 '어절' 어절은 단어와 같지 않다.\n",
    "3. 한국어는 교착어다\n",
    "4. 교착어는 조사,어미를 붙여서 쓰는 말이다.\n",
    "\n",
    "### 한국어(교착어) 특성\n",
    "1. 그 라는 단어에 그가, 그에게, 그를, 그와 등등 다양한 조사가 붙는다.\n",
    "2. 같은 단어인데 다른 조사가 붙어서 다른 단어로 인식이 된다.\n",
    "3. 형태소(morpheme) : 뜻을 가진 가장 작은 말의 단위\n",
    "    1. 자립 형태소 : 그 자체로 단어가 되는 형태소 \n",
    "        - 체언(명사,대명사,수사), 수식언(관형사,부사), 감탄사\n",
    "    2. 의존 형태소 : 다른 형태소와 결합하여 사용되는 형태소\n",
    "        - 접사, 어미, 조사, 어간\n",
    "4. 영어보다 띄어쓰기가 잘 지켜지지 않는다.\n",
    "    - 띄어쓰기를 재대로 하지 않아도 이해가 되는 경우가 많기 때문\n",
    "    - Tobeornottobethaisthequestion\n",
    "    - 제가이렇게띄어씍를전혀하지않고글을썻다고하더라도글을이해할수있습니다.\n",
    "5. 품사 태깅(Part-of-speech tagging)\n",
    "    - fly : 동사로는 '날다', 명사로는'파리'\n",
    "    - 못 : 부사로는 '할수없다', 명사로는 '망치등을 사용해 목재 따위를 고정하는 물건'\n",
    "    - 단어 토큰화 과정에서 각 단어가 어떤 품사로 쓰였는지를 구분해 놓기도 함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tag import pos_tag\n",
    "\n",
    "nltk.download(\"averaged_perceptron_tagger_eng\")\n",
    "text = \"I am actively looking for Ph.D. students. and you are a Ph.D. student.\"\n",
    "tokenized_sentence = word_tokenize(text)\n",
    "\n",
    "print(\"단어 토큰화 :\", tokenized_sentence)\n",
    "print(\"품사 태깅 :\", pos_tag(tokenized_sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Okt\n",
    "from konlpy.tag import Kkma\n",
    "\n",
    "okt = Okt()\n",
    "kkma = Kkma()\n",
    "\n",
    "print(\"OKT 형태소 분석 :\", okt.morphs(\"열심히 코딩한 당신, 연휴에는 여행을 가봐요\"))\n",
    "print(\"OKT 품사 태깅 :\", okt.pos(\"열심히 코딩한 당신, 연휴에는 여행을 가봐요\"))\n",
    "print(\"OKT 명사 추출 :\", okt.nouns(\"열심히 코딩한 당신, 연휴에는 여행을 가봐요\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"꼬꼬마 형태소 분석 :\", kkma.morphs(\"열심히 코딩한 당신, 연휴에는 여행을 가봐요\"))\n",
    "print(\"꼬꼬마 품사 태깅 :\", kkma.pos(\"열심히 코딩한 당신, 연휴에는 여행을 가봐요\"))\n",
    "print(\"꼬꼬마 명사 추출 :\", kkma.nouns(\"열심히 코딩한 당신, 연휴에는 여행을 가봐요\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 09-02 텍스트 데이터의 정제와 정규화\n",
    "1. 토큰화 작업 전, 후에는 텍스트 데이터를 정제(cleaning), 정규화(normalization) 함께 해야함.\n",
    "- cleaning : 갖고 있는 코퍼스로부터 노이즈 데이터 제거\n",
    "- normalization : 표현 방법이 다른 단어들을 통합시켜 같은 단어로 만들어줌\n",
    "\n",
    "2. 규칙에 기반한 표기가 다른 단어 통합\n",
    "- USA, US\n",
    "- uh-huh, uhhuh\n",
    "\n",
    "3. 대, 소문자 통합\n",
    "- 모두 소문자로 통합하면 문제 생김\n",
    "    - US(미국), us(우리)\n",
    "    - General Motors, Bush 회사이름, 사람이름은 대문자 유지\n",
    "- 문장의 첫 단어의 대문자만 소문자로 바꾸는 방법\n",
    "- 훈련에 사용하는 Corpus가 대,소문자 규칙에 어긋나게 썼다면 그냥 소문자로 다 통합하는게 좋을때도 있음\n",
    "\n",
    "4. 불필요한 단어 제거\n",
    "    1. 등장 빈도가 적은 단어\n",
    "    2. 길이가 짧은 단어\n",
    "        - 영어에서 효과적이라고 알려짐\n",
    "        - 한국어 단어는 영어보다 평균적으로 짧음\n",
    "\n",
    "5. 정규 표현식\n",
    "- 규칙기반으로 cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "text = \"I was wondering if anyone out there could enlighten me on this car.\"\n",
    "\n",
    "# 길이가 1~2인 단어들을 정규 표현식을 이용하여 삭제\n",
    "shortword = re.compile(r\"\\W*\\b\\w{1,2}\\b\")\n",
    "print(shortword.sub(\"\", text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 09-03 Stopwords\n",
    "1. 자주 등장하지만 분석에 큰 도움이 되지 않는 단어\n",
    "    - I,my,me,over,조사,접미사"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from konlpy.tag import Okt\n",
    "\n",
    "nltk.download(\"stopwords\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLTK에서 불용어 확인하기 (Natural Language Toolkit)\n",
    "stop_words_list = stopwords.words(\"english\")\n",
    "print(\"불용어 개수 :\", len(stop_words_list))\n",
    "print(\"불용어 10개 출력 :\", stop_words_list[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLTK를 통해서 불용어 제거하기\n",
    "example = \"Family is not an important thing. It's everything.\"\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "word_tokens = word_tokenize(example)\n",
    "\n",
    "result = []\n",
    "for word in word_tokens:\n",
    "    if word not in stop_words:\n",
    "        result.append(word)\n",
    "\n",
    "print(\"불용어 제거 전 :\", word_tokens)\n",
    "print(\"불용어 제거 후 :\", result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "okt = Okt()\n",
    "\n",
    "example = \"고기를 아무렇게나 구우려고 하면 안 돼. 고기라고 다 같은 게 아니거든. 예컨대 삼겹살을 구울 때는 중요한 게 있지.\"\n",
    "stop_words = \"를 아무렇게나 구 우려 고 안 돼 같은 게 구울 때 는\"\n",
    "\n",
    "stop_words = set(stop_words.split(\" \"))\n",
    "word_tokens = okt.morphs(example)\n",
    "\n",
    "result = [word for word in word_tokens if not word in stop_words]\n",
    "\n",
    "print(\"불용어 제거 전 :\", word_tokens)\n",
    "print(\"불용어 제거 후 :\", result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 정규 표현식\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# 1. . 기호\n",
    "# 한개의 임의의 문자를 나타냄 a.c 는 abc, azc, adc , a!c와 같은 형태는 매치\n",
    "r = re.compile(\"a.c\")\n",
    "print(r.search(\"kkk\"))\n",
    "print(r.search(\"abc\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. ? 기호\n",
    "# ?앞의 문자가 존재할수도 존재하지 않을수도 있음을 나타냄\n",
    "# ab?c -> abc, ac 둘다 매치\n",
    "r = re.compile(\"ab?c\")\n",
    "print(r.search(\"abbc\"))\n",
    "print(r.search(\"abc\"))\n",
    "print(r.search(\"ac\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. *기호\n",
    "# *바로앞의 문자가 0개 이상일 경우를 나타냄\n",
    "# 존재하지 않을수도 있고 또는 여러개일수도 있음\n",
    "# ab*c -> ac, abc, abbc, abbbc\n",
    "\n",
    "r = re.compile(\"ab*c\")\n",
    "print(r.search(\"a\"))\n",
    "print(r.search(\"ac\"))\n",
    "print(r.search(\"abc\"))\n",
    "print(r.search(\"abbbbbbbc\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. +기호\n",
    "# *와 유사하나 최소 1개이상\n",
    "\n",
    "r = re.compile(\"ab+c\")\n",
    "print(r.search(\"ac\"))\n",
    "print(r.search(\"abbbbbbbc\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. ^기호\n",
    "# 시작되는 문자열 지정\n",
    "# ^ab -> ab로 시작하는 문자열 매칭\n",
    "\n",
    "r = re.compile(\"^ab\")\n",
    "print(r.search(\"bbc\"))\n",
    "print(r.search(\"abccsad\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. {숫자} 기호\n",
    "# 문자에 {숫자}를 붙이면 해당 문자를 숫자만큼 반복한 것을 나타냄\n",
    "# ab{2}c 라면 a와 c사이에 b가 존재하면서 b가 2개\n",
    "\n",
    "r = re.compile(\"ab{2}c\")\n",
    "print(r.search(\"ac\"))\n",
    "print(r.search(\"abbc\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. {숫자1, 숫자2}기호\n",
    "# 문자에 해당 기호를 붙이면 해당 문자를 숫자1이상~숫자2이하만큼 반복\n",
    "\n",
    "r = re.compile(\"ab{2,8}c\")\n",
    "print(r.search(\"ac\"))\n",
    "print(r.search(\"abbc\"))\n",
    "print(r.search(\"abbbbbbbbc\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. {숫자, } 기호\n",
    "# 문자에 해당 기호를 붙이면 해당 문자를 숫자 이상만큼 반복\n",
    "\n",
    "r = re.compile(\"ab{2,}c\")\n",
    "print(r.search(\"abc\"))\n",
    "print(r.search(\"abbc\"))\n",
    "print(r.search(\"abbbbbbbbbbbc\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9. []기호\n",
    "# []안에 문자들을 넣으면 그 문자들 중 한개의 문자와 매치\n",
    "# [abc] a or b or c가 들어가있는 문자열과 매치\n",
    "# 범위 지정도 가능 [a-zA-Z]는 알파벳 전부\n",
    "# [0-9]는 숫자 전부\n",
    "\n",
    "r = re.compile(\"[abc]\")\n",
    "print(r.search(\"zzz\"))\n",
    "print(r.search(\"a\"))\n",
    "print(r.search(\"aaaaaaa\"))\n",
    "print(r.search(\"bcabca\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = re.compile(\"[a-z]\")\n",
    "print(r.search(\"AAA\"))\n",
    "print(r.search(\"111\"))\n",
    "print(r.search(\"asuFD\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10. [^문자] 기호\n",
    "# ^기호 뒤에 붙은 문자들을 제외한 모든 문자를 매치\n",
    "# [^abc] a or b or c가 들어간 문자열을 제외한 모든 문자열 매치\n",
    "\n",
    "r = re.compile(\"[^abc]\")\n",
    "print(r.search(\"a\"))\n",
    "print(r.search(\"ab\"))\n",
    "print(r.search(\"b\"))\n",
    "print(r.search(\"d\"))\n",
    "print(r.search(\"1\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 정규 표현식 모듈 함수 예제\n",
    "# re.match() 와 re.search()의 차이\n",
    "# search 는 문자열 전체에서 정규표현식과 매치하는지 봄\n",
    "# match 는 문자열 첫 부분부터 매치하는지 확인, 시작 부분이 매치하지 않으면 찾지 않음\n",
    "\n",
    "r = re.compile(\"ab.\")\n",
    "print(r.match(\"kkkabc\"))\n",
    "print(r.search(\"kkkabc\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# re.split()\n",
    "# 정규표현식을 기준으로 문자열들을 분리하려 리스트로 리턴\n",
    "# 공백 기준 분리\n",
    "text = \"사과 딸기 수박 메론 바나나\"\n",
    "re.split(\" \", text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 줄바꿈 기준 분리\n",
    "text = \"\"\"사과\n",
    "딸기\n",
    "수박\n",
    "메론\n",
    "바나나\"\"\"\n",
    "\n",
    "re.split(\"\\n\", text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# '+'를 기준으로 분리\n",
    "text = \"사괴+딸기+수박+메론+바나나\"\n",
    "re.split(\"\\+\", text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# re.findall()\n",
    "# findall()함수는 정규 표현식과 매치되는 모든 문자열들을 리스트로 리턴\n",
    "# 매치되는 문자열 없으면 빈 리스트 리턴\n",
    "\n",
    "text = \"\"\"이름 : 김철수\n",
    "전화번호 : 010 - 1234 - 1234\n",
    "나이 : 30\n",
    "성별 : 남\"\"\"\n",
    "\n",
    "print(re.findall(\"\\d+\", text))  # 전체 텍스트에서 숫자만 찾아서 리턴\n",
    "print(re.findall(\"\\d+\", \"문자열입니다.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# re.sub()\n",
    "# 정규표현식 패턴과 일치하는 문자열 찾아 다른 문자열로 대체\n",
    "text = \"Regular expression : A regular expression, regex or regexp[1] (sometimes called a rational expression)[2][3] is, in theoretical computer science and formal language theory, a sequence of characters that define a search pattern.\"\n",
    "\n",
    "preprocessed_text = re.sub(\"[^a-zA-Z]\", \" \", text)  # 영어알파벳이 아닌것을 공백으로\n",
    "print(preprocessed_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"100 John    PROF\n",
    "101 James   STUD\n",
    "102 Mac   STUD\"\"\"\n",
    "\n",
    "print(re.split(\"\\s+\", text))  # 공백을 찾아내는 정규표현식\n",
    "print(re.findall(\"\\d+\", text))  # 숫자 최소 한개\n",
    "print(\n",
    "    re.findall(\"[A-Z]\", text)\n",
    ")  # 대문자 찾기, 문자열을 가져오는게 아니라 대문자 각각을 가져옴\n",
    "print(re.findall(\"[A-Z]{4}\", text))  # 대문자가 연속 4개이상 등장하는 경우\n",
    "print(re.findall(\"[A-Z][a-z]+\", text))  # 처음에는 대문자 그 뒤에 소문자 여러개"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "text = \"Don't be fooled by the dark sounding name, Mr. Jone's Orphanage is as cheery as cheery goes for a pastry shop\"\n",
    "\n",
    "tokenizer1 = RegexpTokenizer(\"[\\w]+\")  # 문자 또는 숫자가 1개이상인경우\n",
    "tokenizer2 = RegexpTokenizer(\"\\s+\", gaps=True)  # 공백기준\n",
    "\n",
    "print(tokenizer1.tokenize(text))\n",
    "print(tokenizer2.tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 자연어처리 토큰화 -> 단어 집합 생성 -> 정수 인코딩 -> 패딩 -> 벡터화\n",
    "en_text = \"A Dog Run back corner near spare bedrooms\"\n",
    "!pip install spacy\n",
    "!python -m spacy download en_core_web_sm\n",
    "import spacy # 토큰화 라이브러리\n",
    "spacy_en = spacy.load('en_core_web_sm')\n",
    "\n",
    "print(spacy_en.tokenizer(en_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(en_text):\n",
    "    return [tok.text for tok in spacy_en.tokenizer(en_text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenize(en_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install nltk\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt_tab')\n",
    "from nltk.tokenize import word_tokenize\n",
    "print(word_tokenize(en_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 띄어쓰기로 토큰화\n",
    "print(en_text.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 한국어 띄어쓰기 토큰화\n",
    "kor_text = \"사과의 놀라운 효능이라는 글을 봤어. 그래서 오늘 사과를 먹으려고 했는데 사과가 썩어서 슈퍼에 가서 사과랑 오렌지 사왔어\"\n",
    "print(kor_text.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 형태소 토큰화\n",
    "!pip install konlpy\n",
    "!pip install mecab-python\n",
    "!bash <(curl -s https://raw.githubusercontent.com/konlpy/konlpy/master/scripts/mecab.sh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Mecab\n",
    "\n",
    "tokenizer = Mecab()\n",
    "print(tokenizer.morphs(kor_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문자 토큰화\n",
    "print(list(en_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 단어 집합 생성\n",
    "# 중복을 제거한 텍스트의 총 단어의 집합\n",
    "import urllib.request\n",
    "import pandas as pd\n",
    "from konlpy.tag import Mecab\n",
    "from nltk import FreqDist\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urllib.request.urlretrieve(\n",
    "    \"https://raw.githubusercontent.com/e9t/nsmc/master/ratings.txt\",\n",
    "    filename=\"ratings.txt\",\n",
    ")\n",
    "data = pd.read_table(\"ratings.txt\")  # 데이터프레임에 저장\n",
    "data[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"전체 샘플의 수 : {len(data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_data = data[:100]  # 임의로 100개만 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 정규표현식으로 데이터 정제\n",
    "sample_data[\"document\"] = sample_data[\"document\"].str.replace(\n",
    "    \"[^ㄱ-하-ㅣ가-힣]\", \"\", regex=True\n",
    ")\n",
    "# 한글과 공백을 제외하고 모두 제거\n",
    "sample_data[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 불용어 정의\n",
    "stopwords = [\n",
    "    \"의\",\n",
    "    \"가\",\n",
    "    \"이\",\n",
    "    \"은\",\n",
    "    \"들\",\n",
    "    \"는\",\n",
    "    \"좀\",\n",
    "    \"잘\",\n",
    "    \"걍\",\n",
    "    \"과\",\n",
    "    \"도\",\n",
    "    \"를\",\n",
    "    \"으로\",\n",
    "    \"자\",\n",
    "    \"에\",\n",
    "    \"와\",\n",
    "    \"한\",\n",
    "    \"하다\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Mecab()\n",
    "tokenized = []\n",
    "for sentence in sample_data[\"document\"]:\n",
    "    temp = tokenizer.morphs(sentence)  # 토큰화\n",
    "    temp = [word for word in temp if not word in stopwords]  # 불용어 제거\n",
    "    tokenized.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenized[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = FreqDist(np.hstack(tokenized))  # 단어 빈도 계산\n",
    "print(f\"단어 집합의 크기 : {len(vocab)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab[\"재밌\"]  # 단어를 key, 빈도수를 value로"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 500\n",
    "# 상위 vocab_size개의 단어만 보존\n",
    "vocab = vocab.most_common(vocab_size)\n",
    "print(f\"단어 집합의 크기 : {len(vocab)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 각 단어에 고유한 정수 부여\n",
    "word_to_index = {word[0]: index + 2 for index, word in enumerate(vocab)}\n",
    "word_to_index[\"pad\"] = 1\n",
    "word_to_index[\"unk\"] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 기존 훈련 데이터에서 각 단어를 고유한 정수로 부여\n",
    "encoded = []\n",
    "for line in tokenized:  # 입력 데이터에서 1줄씩 문장을 읽음\n",
    "    temp = []\n",
    "    for w in line:  # 각 줄에서 1개씩 글자를 읽음\n",
    "        try:\n",
    "            temp.append(word_to_index[w])  # 글자를 해당되는 정수로 변환\n",
    "        except KeyError:  # 단어 집합에 없는 단어일 경우 unk로 대체\n",
    "            temp.append(word_to_index[\"unk\"])  # unk의 인덱스로 변환\n",
    "    encoded.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(encoded[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 길이가 다른 문장들을 모두 동일한 길이로 바꿔주는 패딩(padding)\n",
    "# 길이가 정해준 길이보다 짧은 샘플들에 'pad'토큰 추가해서 길이 맞춰줌\n",
    "\n",
    "max_len = max(len(l) for l in encoded)\n",
    "print(\"리뷰의 최대 길이 : %d\" % max_len)\n",
    "print(\"리뷰의 최소 길이 : %d\" % min(len(l) for l in encoded))\n",
    "print(\"리뷰의 평균 길이 : %f\" % (sum(map(len, encoded)) / len(encoded)))\n",
    "\n",
    "plt.hist([len(s) for s in encoded], bins=50)\n",
    "plt.xlabel(\"length of sample\")\n",
    "plt.ylabel(\"number of sample\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모든 리뷰 길이 62로 통일\n",
    "for line in encoded:\n",
    "    if len(line) < max_len:\n",
    "        line += [word_to_index[\"pad\"]] * (max_len - len(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"리뷰의 최대 길이 : %d\" % max_len)\n",
    "print(\"리뷰의 최소 길이 : %d\" % min(len(l) for l in encoded))\n",
    "print(\"리뷰의 평균 길이 : %f\" % (sum(map(len, encoded)) / len(encoded)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(encoded[:3])"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 언어 모델(Language Model)이란?\n",
    "- 언어를 모델링하고자 단어 시퀀스(문장)에 확률을 할당하는 모델(가장 자연스러운 단어 시퀀스를 찾아내는 모델)\n",
    "    1. 통계를 이용한 방법\n",
    "    2. 인공 신경망을 이용한 방법 -> 최근 주로 사용\n",
    "- 이전 단어들이 주어졌을 때 다음 단어 예측\n",
    "\n",
    "## 통계적 언어 모델(Statistical Language Model, SLM)\n",
    "- 문장의 확률을 구하기 위해 문장을 구성하는 단어들의 다음 단어에 대한 예측 확률을 모두 곱함\n",
    "- 확률 계산 방법 : 데이터 기반 카운트 (이전 데이터에서 이 단어가 다음 단어로 몇번 나왔느냐)\n",
    "### 카운트 기반 접근의 한계 - 희소 문제(Sparsity Problem)\n",
    "- 기계가 학습할 문장의 양이 엄청 많아야함.\n",
    "- 기존 학습한 데이터에 그 문장이 없으면 확률 0으로 나옴\n",
    "- 데이터가 충분하지 못해 관측값을 못찾아 모델링 제대로 안되는 문제를 Sparsity problem이라고 함.\n",
    "- 이러한 문제로 통계적 언어모델에서 신경망 언어모델로 넘어가게 됨\n",
    "\n",
    "### N-gram\n",
    "- SLM의 일종이나 이전에 등장한 모든 단어를 고려하는 것이 아니라 일부 단어만 고려하는 접근 방법\n",
    "- 일부 단어를 몇 개 보느냐가 N-gram에서의 N\n",
    "- 다음에 나올 단어의 예측에 이전 N-1개의 단어에만 의존\n",
    "#### N-gram의 한계\n",
    "- 전체 문맥 고려하기 힘듬\n",
    "- 여전히 희소 문제 존재함.\n",
    "- n을 결정하는데에 trade-off 문제\n",
    "    - n이 너무 작으면 전체 문맥 파악 못함\n",
    "    - 너무 크면 희소 문제 심각해짐, 모델 사이즈도 커짐\n",
    "    - n이 5 넘어가면 안된다고 권장\n",
    "\n",
    "### Perplexity(PPL)\n",
    "- 언어 모델 성능 평가 방법 \n",
    "- perplexed(헷갈리는) -> 낮을수록 성능이 좋은 것\n",
    "- 분기 계수 (Branching factor)\n",
    "    - 선택할 수 이쓴ㄴ 가능한 경우의 수\n",
    "    - 언어 모델이 특정 시점에서 평균적으로 몇 개의 선택지를 가지고 고민하고 있는지\n",
    "    - PPL이 10이라는 뜻은 다음 단어를 예측하는 모든 시점마다 평균 10개의 단어를 가지고 고민한다.\n",
    "    - PPL이 낮다는것은 테스트 데이터에 높은 정확도를 보인다는 뜻. -> 사람이 느끼기에 좋은 언어 모델이라는 말은 아니다.\n",
    "\n",
    "## 텍스트의 유사도(Text similarity)\n",
    "### 단어의 표현 방법\n",
    "1. 국소 표현(Local) : 해당 단어 자체만 보고 특정값 맵핑\n",
    "    - 이산 표현(Disccrete)라고도 함.\n",
    "2. 분산 표현(Distributed) : 주변 단어를 참고하여 단어를 표현\n",
    "    - 분산 표현 방법은 주변단어를 참고하므로 의미,뉘앙스를 표현 할 수 있음\n",
    "    - 연속 표현(Continuous)라고도 함\n",
    "\n",
    "![image.png](attachment:image.png)\n",
    "\n",
    "### Bag of Words\n",
    "- 단어 순서는 전혀 고려하지 않음.\n",
    "- 출현 빈도(frequency)만 집중\n",
    "#### BoW 만드는 과정\n",
    "1. 각 단어에 고유한 정수 인덱스를 부여합니다.  # 단어 집합 생성.\n",
    "2. 각 인덱스의 위치에 단어 토큰의 등장 횟수를 기록한 벡터를 만듭니다.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BoW 예제\n",
    "\n",
    "from konlpy.tag import Okt\n",
    "\n",
    "okt = Okt()\n",
    "\n",
    "\n",
    "def build_bag_of_words(document):\n",
    "    # 온점 제거 및 형태소 분석\n",
    "    document = document.replace(\".\", \"\")\n",
    "    tokenized_document = okt.morphs(document)\n",
    "\n",
    "    word_to_index = {}\n",
    "    bow = []\n",
    "\n",
    "    for word in tokenized_document:\n",
    "        if word not in word_to_index.keys():\n",
    "            word_to_index[word] = len(word_to_index)\n",
    "            # BoW에 전부 기본값 1을 넣는다.\n",
    "            bow.insert(len(word_to_index) - 1, 1)\n",
    "        else:\n",
    "            # 재등장하는 단어의 인덱스\n",
    "            index = word_to_index.get(word)\n",
    "            # 재등장한 단어는 해당하는 인덱스의 위치에 1을 더한다.\n",
    "            bow[index] = bow[index] + 1\n",
    "    return word_to_index, bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc1 = \"정부가 발표하는 물가상승률과 소비자가 느끼는 물가상승률은 다르다.\"\n",
    "vocab, bow = build_bag_of_words(doc1)\n",
    "print(\"vocabulary :\", vocab)\n",
    "print(\"bag of words vector :\", bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc2 = \"소비자는 주로 소비하는 상품을 기준으로 물가상승률을 느낀다.\"\n",
    "\n",
    "vocab, bow = build_bag_of_words(doc2)\n",
    "print(\"vocabulary :\", vocab)\n",
    "print(\"bag of words vector :\", bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc3 = doc1 + \" \" + doc2\n",
    "vocab, bow = build_bag_of_words(doc3)\n",
    "print(\"vocabulary :\", vocab)\n",
    "print(\"bag of words vector :\", bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CountVectorizer 클래스로 BoW만들기\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "corpus = [\"you know I want your love. because I love you.\"]\n",
    "vector = CountVectorizer()\n",
    "\n",
    "# 코퍼스로부터 각 단어의 빈도수를 기록\n",
    "print(\"bag of words vector :\", vector.fit_transform(corpus).toarray())\n",
    "\n",
    "# 각 단어의 인덱스가 어떻게 부여되었는지를 출력\n",
    "print(\"vocabulary :\", vector.vocabulary_)\n",
    "# 길이 2이상인 문자에 대해서만 토큰으로 인식해 I 사라짐\n",
    "# 띄어쓰기만을 기준으로 토큰화 -> 한국어에 적용하면 재대로 BoW 만들 수 없음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 불용어 제거 후 BoW 만들기\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# 1. 직접 정의한 불용어 사용\n",
    "text = [\"Family is not an important thing. It's everything.\"]\n",
    "vect = CountVectorizer(stop_words=[\"the\", \"a\", \"an\", \"is\", \"not\"])\n",
    "print(\"bag of words vector :\", vect.fit_transform(text).toarray())\n",
    "print(\"vocabulary :\", vect.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CounterVectorizer에서 제공하는 자체 불용어 사용\n",
    "text = [\"Family is not an important thing. It's everything.\"]\n",
    "vect = CountVectorizer(stop_words=\"english\")\n",
    "print(\"bag of words vector :\", vect.fit_transform(text).toarray())\n",
    "print(\"vocabulary :\", vect.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLTK에서 지원하는 불용어 사용\n",
    "text = [\"Family is not an important thing. It's everything.\"]\n",
    "stop_words = stopwords.words(\"english\")\n",
    "vect = CountVectorizer(stop_words=stop_words)\n",
    "print(\"bag of words vector :\", vect.fit_transform(text).toarray())\n",
    "print(\"vocabulary :\", vect.vocabulary_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DTM과 TF-IDF 행렬\n",
    "### Document-Term Matrix,(DTM) : 서로 다른 문서들의 BoW들을 결합한 표현 방법, 행과열 반대로 선택하면 TDM\n",
    "- 다수의 문서에서 등장하는 각 단어들의 빈도를 행렬로 표현한 것\n",
    "- 각 문서에 대한 BoW를 하나의 행렬로 만든 것\n",
    "#### DTM의 한계\n",
    "1. Sparse representation\n",
    "2. 단순 빈도 수 기반 접근 : 빈도가 많다고 중요한 단어가 아닌 경우 많다.(The, a 등등)\n",
    "\n",
    "### TF-IDF(Term Frequency-Inverse Document Frequency)\n",
    "- 문서 : d\n",
    "- 단어 : t\n",
    "- 문서의 총 갯수 : n\n",
    "1. TF(d,t) : 특성 문서 d에서 특정 단어 t의 등장 횟수 -> DTM\n",
    "2. DF(t) : 특정 단어 t가 등장한 문서 수 -> 여러번 등장했다고 해도 한번만 체크\n",
    "3. IDF(t) : DF(t)에 반비례하는 수 : log(n/(1+DF(t)))\n",
    "    - log를 사용하지 않으면 n이 커지면 IDF값이 기하급수적으로 커진다\n",
    "    - 단어들마다 출현 빈도가 엄청 차이나기 떄문에 log 안하면 희귀 단어들에 엄청난 가중치 부여 가능\n",
    "    - 1 더하는 이유는 DF(t) = 0 인 경우 고려\n",
    "    - 모든 문서에서 자주 등장하는 단어는 중요도 낮음, 특정 문서에서만 자주 등장하는 단어는 중요도 높음\n",
    "    - TF-IDF 값이 낮으면 중요도 낮은것"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF 직접 구현\n",
    "import pandas as pd\n",
    "from math import log\n",
    "\n",
    "docs = [\n",
    "    \"먹고 싶은 사과\",\n",
    "    \"먹고 싶은 바나나\",\n",
    "    \"길고 노란 바나나 바나나\",\n",
    "    \"저는 과일이 좋아요\",\n",
    "]\n",
    "vocab = list(set(w for doc in docs for w in doc.split()))\n",
    "vocab.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 총 문서의 수\n",
    "N = len(docs)\n",
    "\n",
    "\n",
    "def tf(t, d):\n",
    "    return d.count(t)\n",
    "\n",
    "\n",
    "def idf(t):\n",
    "    df = 0\n",
    "    for doc in docs:\n",
    "        df += t in doc  # doc 안에 단어 t가 있으면 True(1), 없으면 False(0)가 더해짐\n",
    "    return log(N / (df + 1))\n",
    "\n",
    "\n",
    "def tfidf(t, d):\n",
    "    return tf(t, d) * idf(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF를 저장할 빈 리스트 초기화\n",
    "result = []\n",
    "\n",
    "# 각 문서에 대해서 TF 값 계산\n",
    "for i in range(N):  # 문서 수 만큼 반복\n",
    "    result.append([])  # i번째 TF 값을 저장할 빈 리스트 추가\n",
    "    d = docs[i]  # 현재 처리할 문서를 변수 d에 할당\n",
    "    for j in range(len(vocab)):  # 단어 집합 갯수만큼 반복\n",
    "        t = vocab[j]  # 현재 처리할 단어를 변수 t에 할당\n",
    "        result[-1].append(\n",
    "            tf(t, d)\n",
    "        )  # tf함수로 TF값계산, result의 마지막 리스트(현재 문서의 행)에 추가\n",
    "\n",
    "tf_ = pd.DataFrame(result, columns=vocab)\n",
    "print(tf_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IDF를 저장할 빈 리스트 초기화\n",
    "result = []\n",
    "for j in range(len(vocab)):\n",
    "    t = vocab[j]  # 현재 처리할 단어 t에 할당\n",
    "    result.append(idf(t))\n",
    "\n",
    "idf_ = pd.DataFrame(result, index=vocab, columns=[\"IDF\"])\n",
    "print(idf_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF 값 저장할 빈 리스트 초기화\n",
    "result = []\n",
    "for i in range(N):\n",
    "    result.append([])\n",
    "    d = docs[i]  # 현재 처리할 문서\n",
    "    for j in range(len(vocab)):\n",
    "        t = vocab[j]  # 현재 처리할 단어\n",
    "        result[-1].append(tfidf(t, d))\n",
    "\n",
    "tfidf_ = pd.DataFrame(result, columns=vocab)\n",
    "tfidf_\n",
    "\n",
    "# 이렇게 구현하면 문제가 IDF값이 0 이 되는 경우가 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 사이킷런을 이용한 DTM TF-IDF\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "corpus = [\"you know I want your love\", \"I like you\", \"what should I do\"]\n",
    "\n",
    "vector = CountVectorizer()\n",
    "\n",
    "# 코퍼스로부터 각 단어의 빈도수를 기록\n",
    "print(vector.fit_transform(corpus).toarray())\n",
    "\n",
    "# 각 단어와 맵핑된 인덱스 출력\n",
    "print(vector.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "corpus = [\"you know I want your love\", \"I like you\", \"what should I do\"]\n",
    "\n",
    "tfidfv = TfidfVectorizer().fit(corpus)\n",
    "print(tfidfv.transform(corpus).toarray())\n",
    "print(tfidfv.vocabulary_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 코사인 유사도를 이용한 추천시스템\n",
    "\n",
    "문서1 : 저는 사과 좋아요\n",
    "문서2 : 저는 바나나 좋아요\n",
    "문서3 : 저는 바나나 좋아요 저는 바나나 좋아요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.linalg import norm\n",
    "\n",
    "\n",
    "def cos_sim(A, B):\n",
    "    return np.dot(A, B) / (norm(A) * norm(B))\n",
    "\n",
    "\n",
    "doc1 = np.array([0, 1, 1, 1])\n",
    "doc2 = np.array([1, 0, 1, 1])\n",
    "doc3 = np.array([2, 0, 2, 2])\n",
    "\n",
    "print(\"문서 1과 문서2의 유사도 :\", cos_sim(doc1, doc2))\n",
    "print(\"문서 1과 문서3의 유사도 :\", cos_sim(doc1, doc3))\n",
    "print(\"문서 2와 문서3의 유사도 :\", cos_sim(doc2, doc3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 유사도를 이용한 추천 시스템 구현\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "data = pd.read_csv(\"movies_metadata.csv\", low_memory=False)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.head(20000)  # 상위 2만개만 사용\n",
    "print(\"overview 열의 결측값의 수:\", data[\"overview\"].isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 결측값을 빈 값으로 대체\n",
    "data[\"overview\"] = data[\"overview\"].fillna(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(stop_words=\"english\")\n",
    "tfidf_matrix = tfidf.fit_transform(data[\"overview\"])\n",
    "print(\"TF-IDF 행렬의 크기(shape) :\", tfidf_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_sim = cosine_similarity(tfidf_matrix, tfidf_matrix)\n",
    "print(\"코사인 유사도 연산 결과 :\", cosine_sim.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 영화 title을 key, 영화 index를 value로 하는 dict 생성\n",
    "title_to_index = dict(zip(data[\"title\"], data.index))\n",
    "\n",
    "# 영화 제목 Father of the Bride Part II의 인덱스를 리턴\n",
    "idx = title_to_index[\"Father of the Bride Part II\"]\n",
    "print(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 영화 제목을 입력하면 코사인 유사도를 통해 가장 overview가 유사한 10개의 영화를 찾아내는 함수를 생성\n",
    "\n",
    "\n",
    "def get_recommendations(title, cosine_sim=cosine_sim):\n",
    "    # 선택한 영화의 타이틀로부터 해당 영화의 인덱스를 바당온다.\n",
    "    idx = title_to_index[title]\n",
    "\n",
    "    # 해당 영화와 모든 영화와의 유사도를 가져온다.\n",
    "    sim_scores = list(enumerate(cosine_sim[idx]))\n",
    "\n",
    "    # 유사도에 따라 영화들을 정렬한다.\n",
    "    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # 가장 유사한 10개의 영화를 받아온다.\n",
    "    sim_scores = sim_scores[1:11]\n",
    "\n",
    "    # 가장 유사한 10개의 영화의 인덱스를 받는다\n",
    "    movie_indices = [idx[0] for idx in sim_scores]\n",
    "\n",
    "    # 가장 유사한 10개의 영화의 제목을 리턴한다.\n",
    "    return data[\"title\"].iloc[movie_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_recommendations('The Dark Knight Rises')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11-05 단어와 문서의 유사도를 구하는 다양한 방법\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. 유클리드 거리(Euclidean distance)\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def dist(x,y):   \n",
    "    return np.sqrt(np.sum((x-y)**2))\n",
    "\n",
    "doc1 = np.array((2,3,0,1))\n",
    "doc2 = np.array((1,2,3,1))\n",
    "doc3 = np.array((2,1,2,2))\n",
    "docQ = np.array((1,1,0,1))\n",
    "\n",
    "print('문서1과 문서Q의 거리 :',dist(doc1,docQ))\n",
    "print('문서2과 문서Q의 거리 :',dist(doc2,docQ))\n",
    "print('문서3과 문서Q의 거리 :',dist(doc3,docQ))\n",
    "# 유클리드 거리가 작을수록 가깝다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. 자카드 유사도(Jaccard similarity)\n",
    "# 합집합에서 교집합의 비율\n",
    "doc1 = \"apple banana everyone like likey watch card holder\"\n",
    "doc2 = \"apple banana coupon passport love you\"\n",
    "\n",
    "# 토큰화\n",
    "tokenized_doc1 = doc1.split()\n",
    "tokenized_doc2 = doc2.split()\n",
    "\n",
    "print('문서1 :',tokenized_doc1)\n",
    "print('문서2 :',tokenized_doc2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문서1 , 문서2 합집합 계산\n",
    "union = set(tokenized_doc1).union(set(tokenized_doc2))\n",
    "print('문서1과 문서2의 합집합 :', union)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문서1, 문서 2 교집합\n",
    "intersection = set(tokenized_doc1).intersection(set(tokenized_doc2))\n",
    "print('문서1과 문서2의 교집합 :', intersection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('자카드 유사도 :', len(intersection)/len(union))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. 텍스트 임베딩(Embedding) : 신경망 기반 표현 방법"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Okt\n",
    "okt = Okt()\n",
    "token = okt.morphs(\"나는 자연어 처리를 배운다\")\n",
    "print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2index={}\n",
    "for voca in token:\n",
    "    if voca not in word2index.keys():\n",
    "        word2index[voca] = len(word2index)\n",
    "print(word2index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encoding(word, word2index):\n",
    "    one_hot_vector = [0] * (len(word2index))\n",
    "    index = word2index[word]\n",
    "    one_hot_vector[index] = 1 \n",
    "    return one_hot_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot_encoding(\"자연어\",word2index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 밀집 표현(Dense Representation)\n",
    "- 벡터의 차원을 사용자가 설정한 값으로 맞춤\n",
    "## 워드 임베딩(Word Embedding)\n",
    "- 단어를 밀집 벡터의 형태로 표현하는 방법\n",
    "### 분산 표현(Distributed Representation)\n",
    "- 분포 가설 : 비슷한 위치에서 등장하는 단어들은 비슷한 의미를 가진다.\n",
    "### CBOW(Continuous Bag of Words)\n",
    "- 주변에 있는 단어들을 가지고, 중간에 있는 단어들을 예측하는 방법\n",
    "- Input layer : 사용자가 정한 윈도우 크기 범위 안에 있는 주변단어들의 원-핫 벡터\n",
    "- Projection layer : Lookup table\n",
    "    - 투사층의 크기 : M (임베딩하고 난 벡터의 차원)\n",
    "    - 입력층과 투사층 사이 가중치 W = V x M (V = 단어 집합 크기)\n",
    "    - 투사층과 출력층 사이 가중치 W'= M x V\n",
    "    - 가중치는 서로 다르다.\n",
    "    - 입력으로 들어오는 주변 단어의 원-핫 벡터와 가중치 W 행렬 곱은 사실 W행렬의 i번째 행을 그대로 읽어오는것과 같다.\n",
    "    - 그래서 lookup table\n",
    "    - 투사층에서 입력과 W 곱 벡터 평균함.\n",
    "    - 평균된 벡터를 다시 W'와 곱하면 입력벡터의 차원으로 변경\n",
    "    - 이 벡터에 softmax 함수 취하면 j번째 단어가 중심 단어일 확률 나타냄\n",
    "    - 정답 데이터와의 오차를 줄이기 위해 cross-entropy를 손실함수로 하여 학습함.\n",
    "    - 오차역전파로 학습하고 학습이 다 되면 W,W'중에 어떤걸 임베딩 벡터로 사용할지 결정, 평균으로 하는 경우도 있음.\n",
    "- Output layer : 정답 단어의 원-핫 벡터 필요\n",
    "### Skip-gram\n",
    "- 중심 단어에서 주변단어 예측\n",
    "- Skip-gram이 CBOW보다 성능 좋다고 알려져 있음.\n",
    "### Negative sampling\n",
    "- 보통 Word2Vec은 SGNS(Skip-Gram with Negative sampling)을 말함\n",
    "- Word2Vec의 단점이 속도가 느리다. 출력층에 소프트 맥스 함수로 모든 단어 집합 크기의 벡터를 계산\n",
    "- 단어 벡터가 엄청 크면 시간 오래 걸린다.\n",
    "- 또 이렇게 계산된 값으로 역전파를 수행하면 상관없는 단어들의 가중치도 업데이트가 된다.\n",
    "- 작은 단어 집합을 만들고 주변 단어를 positive 랜덤으로 샘플링 된 단어들을 주변 단어 아니라고 가정하고 negative로 둠\n",
    "- 단어가 positive인지 negative인지 이진 분류"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 12-04 Word2Vec 학습 및 시각화\n",
    "# 1. 영어 Word2Vec 만들기\n",
    "\n",
    "import re\n",
    "import urllib.request\n",
    "import zipfile\n",
    "from lxml import etree\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 훈련 데이터 이해하기 \n",
    "# 데이터 다운로드\n",
    "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/ukairia777/tensorflow-nlp-tutorial/main/09.%20Word%20Embedding/dataset/ted_en-20160408.xml\", filename=\"ted_en-20160408.xml\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 훈련 데이터 전처리 \n",
    "targetXML = open('ted_en-20160408.xml', 'r', encoding='UTF8')\n",
    "target_text = etree.parse(targetXML)\n",
    "\n",
    "# xml 파일로부터 <content>와 </content> 사이의 내용만 가져온다.\n",
    "parse_text = '\\n'.join(target_text.xpath('//content/text()'))\n",
    "\n",
    "# 정규 표현식의 sub 모듈을 통해 content 중간에 등장하는 (Audio), (Laughter)등의 배경음 부분을 제거.\n",
    "# 해당 코드는 괄호로 구성된 내용을 제거.\n",
    "content_text = re.sub(r'\\([^)]*\\)','', parse_text)\n",
    "\n",
    "# 입력 코퍼스에 대해서 NLTK를 이용하여 문장 토큰화를 수행.\n",
    "sent_text = sent_tokenize(content_text)\n",
    "\n",
    "# 각 문장에 대해서 구두점을 제거하고, 대문자를 소무낮로 변환.\n",
    "normalized_text = []\n",
    "for string in sent_text:\n",
    "    tokens = re.sub(r\"[^a-z0-9]+\", \" \", string.lower())\n",
    "    normalized_text.append(tokens)\n",
    "\n",
    "# 각 문장에 대해서 NLTK를 이용하여 단어 토큰화를 수행.\n",
    "result = [word_tokenize(sentence) for sentence in normalized_text]\n",
    "\n",
    "print(f'총 샘플의 개수 : {len(result)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 샘플 3개 출력\n",
    "for line in result[:3]:\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install numpy\n",
    "!pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word2Vec 훈련시키기\n",
    "from gensim.models import Word2Vec, KeyedVectors\n",
    "\n",
    "model = Word2Vec(sentences=result, vector_size=100, window=5, min_count=5, workers=4, sg=0)\n",
    "# vector_size = 임베딩 된 벡터 차원\n",
    "# window = 컨텍스트 윈도우 크기\n",
    "# min_count = 단어 최소 빈도 수 제한(이거보다 빈도가 적은 단어는 학습 안함)\n",
    "# workers = 학습을 위한 프로세스 수\n",
    "# sg = 0 CBOW, 1은 Skip-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_result = model.wv.most_similar(\"man\")\n",
    "print(model_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 저장하고 로드하기\n",
    "model.wv.save_word2vec_format('eng_w2v') # 모델 저장\n",
    "loaded_model = KeyedVectors.load_word2vec_format(\"eng_w2v\") # 모델 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_result = loaded_model.most_similar(\"man\")\n",
    "print(model_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install \"numpy<2.0\" \"scipy<1.14\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 한국어 Word2Vec\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import urllib.request\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from konlpy.tag import Okt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/e9t/nsmc/master/ratings.txt\", filename=\"ratings.txt\")\n",
    "train_data = pd.read_table('ratings.txt')\n",
    "train_data[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(train_data)) # 리뷰 개수 출력\n",
    "print(train_data.isnull().sum()) # 결측값 개수 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data.dropna(how='any') # Null 값이 존재하는 행 제거\n",
    "print(train_data.isnull().sum())\n",
    "print(len(train_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 정규표현식으로 한글 외 문자 제거\n",
    "train_data['document'] = train_data['document'].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣]\",\"\",regex=True)\n",
    "train_data[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from konlpy.tag import Okt\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 불용어 정의\n",
    "stopwords = {'의','가','이','은','들','는','좀','잘','걍','과','도','를','으로','자','에','와','한','하다'}\n",
    "\n",
    "# 형태소 분석기 OKT를 사용한 토큰화 작업 (다소 시간 소요)\n",
    "\n",
    "okt = Okt()\n",
    "\n",
    "tokenized_data = []\n",
    "for sentence in tqdm(train_data['document']):\n",
    "    tokenized_sentence = okt.morphs(sentence, stem=True) # 토큰화\n",
    "    stopwords_removed_sentence = [word for word in tokenized_sentence if not word in stopwords] # 불용어 제거\n",
    "    tokenized_data.append(stopwords_removed_sentence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 리뷰 길이 분포 확인\n",
    "print('리뷰의 최대 길이 :', max(len(review) for review in tokenized_data))\n",
    "print('리뷰의 평균 길이 :', sum(map(len, tokenized_data))/len(tokenized_data))\n",
    "plt.hist([len(review) for review in tokenized_data], bins=50)\n",
    "plt.xlabel('length of samples')\n",
    "plt.ylabel('number of samples')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "model=Word2Vec(sentences = tokenized_data, vector_size = 100, window=5, min_count = 5, workers = 4, sg=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.wv.most_similar(\"최민식\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.wv.most_similar(\"히어로\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[===================-------------------------------] 39.7% 660.0/1662.8MB downloaded"
     ]
    },
    {
     "ename": "ContentTooShortError",
     "evalue": "<urlopen error retrieval incomplete: got only 692060160 out of 1743563840 bytes>",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mContentTooShortError\u001b[39m                      Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgensim\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdownloader\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mapi\u001b[39;00m\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# 'word2vec-google-news-300' 모델을 로드합니다.\u001b[39;00m\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# 로컬에 파일이 없으면 자동으로 다운로드합니다.\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m word2vec_model = \u001b[43mapi\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mword2vec-google-news-300\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# 모델이 성공적으로 로드되었는지 테스트\u001b[39;00m\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     14\u001b[39m     \u001b[38;5;66;03m# 'king'과 가장 유사한 단어들을 출력\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/projects/my_python_project/DL_pytorch/.venv/lib/python3.11/site-packages/gensim/downloader.py:496\u001b[39m, in \u001b[36mload\u001b[39m\u001b[34m(name, return_path)\u001b[39m\n\u001b[32m    494\u001b[39m path = os.path.join(folder_dir, file_name)\n\u001b[32m    495\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os.path.exists(folder_dir):\n\u001b[32m--> \u001b[39m\u001b[32m496\u001b[39m     \u001b[43m_download\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    498\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m return_path:\n\u001b[32m    499\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m path\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/projects/my_python_project/DL_pytorch/.venv/lib/python3.11/site-packages/gensim/downloader.py:396\u001b[39m, in \u001b[36m_download\u001b[39m\u001b[34m(name)\u001b[39m\n\u001b[32m    394\u001b[39m fname = \u001b[33m\"\u001b[39m\u001b[38;5;132;01m{fname}\u001b[39;00m\u001b[33m.gz\u001b[39m\u001b[33m\"\u001b[39m.format(fname=name)\n\u001b[32m    395\u001b[39m dst_path = os.path.join(tmp_dir, fname)\n\u001b[32m--> \u001b[39m\u001b[32m396\u001b[39m \u001b[43murllib\u001b[49m\u001b[43m.\u001b[49m\u001b[43murlretrieve\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdst_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreporthook\u001b[49m\u001b[43m=\u001b[49m\u001b[43m_progress\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    397\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m _calculate_md5_checksum(dst_path) == _get_checksum(name):\n\u001b[32m    398\u001b[39m     sys.stdout.write(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.pyenv/versions/3.11.9/lib/python3.11/urllib/request.py:280\u001b[39m, in \u001b[36murlretrieve\u001b[39m\u001b[34m(url, filename, reporthook, data)\u001b[39m\n\u001b[32m    277\u001b[39m                 reporthook(blocknum, bs, size)\n\u001b[32m    279\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m size >= \u001b[32m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m read < size:\n\u001b[32m--> \u001b[39m\u001b[32m280\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m ContentTooShortError(\n\u001b[32m    281\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mretrieval incomplete: got only \u001b[39m\u001b[38;5;132;01m%i\u001b[39;00m\u001b[33m out of \u001b[39m\u001b[38;5;132;01m%i\u001b[39;00m\u001b[33m bytes\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    282\u001b[39m         % (read, size), result)\n\u001b[32m    284\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[31mContentTooShortError\u001b[39m: <urlopen error retrieval incomplete: got only 692060160 out of 1743563840 bytes>"
     ]
    }
   ],
   "source": [
    "#사전 훈련된 Word2Vec 임베딩\n",
    "\n",
    "import gensim\n",
    "import urllib.request\n",
    "\n",
    "import gensim.downloader as api\n",
    "\n",
    "# 'word2vec-google-news-300' 모델을 로드합니다.\n",
    "# 로컬에 파일이 없으면 자동으로 다운로드합니다.\n",
    "word2vec_model = api.load('word2vec-google-news-300')\n",
    "\n",
    "# 모델이 성공적으로 로드되었는지 테스트\n",
    "try:\n",
    "    # 'king'과 가장 유사한 단어들을 출력\n",
    "    similar_words = word2vec_model.most_similar('king')\n",
    "    print(\"모델이 성공적으로 로드되었습니다.\")\n",
    "    print(\"'king'과 유사한 단어:\", similar_words)\n",
    "except Exception as e:\n",
    "    print(f\"오류가 발생했습니다: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GloVE(Global Vectors for Word Representation)\n",
    "- 카운트 기반(LSA), 예측 기반 모두 사용(Word2Vec)\n",
    "- Window based Co-occurence Matrix \n",
    "    - 행과 열을 전체 단어 집합의 단어들로 구성\n",
    "    - i 단어의 Window Size 내에서 k 단어가 등장한 횟수를 i행 k열에 기재한 행렬\n",
    "    - transpose해도 동일한 행렬\n",
    "- Co-occurrence Probability\n",
    "    - P(k|i) : 특정 단어 i 가 등장했을때 어떤 단어 k 가 등장한 횟수\n",
    "- ** 임베딩 된 중심 단어와 주변 단어 벡터의 내적이 전체 코퍼스에서의 동시 등장 확률이 되도록 만드는것 **\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FastText\n",
    "- 각 단어는 글자 단위 n-gram의 구성으로 취급\n",
    "- apple ->  n = 3인 경우 <ap, app, ppl, ple, le>, <apple>\n",
    "### 장점\n",
    "- Out of Vocabulary에 대한 대응 가능\n",
    "- 단어 집합 내 빈도 수 적었던 단어(Rare Word)에 대한 대응\n",
    "- 오타나 맞춤법 틀린 단어에도 잘 대응 가능\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'to': 2, 'know': 3, 'code': 4, 'you': 5, 'need': 6, 'how': 7, '<unk>': 0, '<pad>': 1}\n"
     ]
    }
   ],
   "source": [
    "# PyTorch의 nn.Embedding\n",
    "# 파이토치는 입력을 원-핫 벡터가 아니여도 룩업테이블 된 결과인 임베딩 벡터 리턴\n",
    "\n",
    "# 룩업 테이블 과정 구현x\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "train_data = 'you need to know how to code'\n",
    "# 중복을 제거한 단어들의 집합인 단어 집합 생성.\n",
    "word_set = set(train_data.split())\n",
    "\n",
    "# 단어 집합의 각 단어에 고유한 정수 맵핑.\n",
    "vocab = {word: i+2 for i, word in enumerate(word_set)}\n",
    "vocab['<unk>'] = 0\n",
    "vocab['<pad>'] = 1\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 단어 집합의 크기만큼의 행을 가지는 테이블 생성.\n",
    "# 임베딩 벡터 차원 3\n",
    "embedding_table = torch.FloatTensor([\n",
    "                               [ 0.0,  0.0,  0.0],\n",
    "                               [ 0.0,  0.0,  0.0],\n",
    "                               [ 0.2,  0.9,  0.3],\n",
    "                               [ 0.1,  0.5,  0.7],\n",
    "                               [ 0.2,  0.1,  0.8],\n",
    "                               [ 0.4,  0.1,  0.1],\n",
    "                               [ 0.1,  0.8,  0.9],\n",
    "                               [ 0.6,  0.1,  0.1]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.4000, 0.1000, 0.1000],\n",
      "        [0.1000, 0.8000, 0.9000],\n",
      "        [0.2000, 0.9000, 0.3000],\n",
      "        [0.0000, 0.0000, 0.0000]])\n"
     ]
    }
   ],
   "source": [
    "sample = 'you need to run'.split()\n",
    "idxes = []\n",
    "\n",
    "# 각 단어를 정수로 변환\n",
    "for word in sample:\n",
    "    try:\n",
    "        idxes.append(vocab[word])\n",
    "    # 단어 집합에 없는 단어일 경우 <unk>로 대체\n",
    "    except KeyError:\n",
    "        idxes.append(vocab['<unk>'])\n",
    "idxes = torch.LongTensor(idxes)\n",
    "\n",
    "# 각 정수를 인덱스로 임베딩 테이블에서 값을 가져온다.\n",
    "lookup_result = embedding_table[idxes, :]\n",
    "print(lookup_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nn.Embedding() 사용하기\n",
    "train_data = 'you need to know how to code'\n",
    "# 중복을 제거한 단어들의 집합인 단어 집합 생성\n",
    "word_set = set(train_data.split())\n",
    "\n",
    "# 단어 집합의 각 단어에 고유한 정수 맵핑.\n",
    "vocab = {tkn: i+2 for i, tkn in enumerate(word_set)}\n",
    "vocab['<unk>'] = 0\n",
    "vocab['<pad>'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 1.7801e-01, -2.1372e+00, -4.4357e-01],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
      "        [ 4.5106e-01,  1.2438e+00,  9.6275e-02],\n",
      "        [-4.8722e-01, -3.3844e-01, -1.1162e-03],\n",
      "        [-1.3288e+00,  8.6437e-01,  1.9920e-01],\n",
      "        [-1.0573e+00, -3.2575e-01,  1.0778e+00],\n",
      "        [ 7.7972e-01, -4.2653e-01, -1.5739e+00],\n",
      "        [-3.2871e-01,  6.4596e-01, -9.7297e-01]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "embedding_layer = nn.Embedding(num_embeddings=len(vocab),\n",
    "                               embedding_dim=3,\n",
    "                               padding_idx = 1)\n",
    "# num_embeddings : 임베딩 할 단어들의 개수 -> 단어 집합 크기\n",
    "# embedding_dim : 임베딩 할 벡터의 차원 -> 하이퍼 파라미터\n",
    "# padding_idx : 선택적으로 사용하는 인자. 패딩을 위한 토큰의 인덱스\n",
    "\n",
    "print(embedding_layer.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문장 긍,부정 판단하는 감성 분류 모델\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import gensim\n",
    "\n",
    "sentences = ['nice great best amazing', 'stop lies', 'pitiful nerd', 'excellent work', 'supreme quality', 'bad', 'highly respectable']\n",
    "y_train = [1, 0, 0, 1, 1, 0, 1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어 토큰화 된 결과 : [['nice', 'great', 'best', 'amazing'], ['stop', 'lies'], ['pitiful', 'nerd'], ['excellent', 'work'], ['supreme', 'quality'], ['bad'], ['highly', 'respectable']]\n"
     ]
    }
   ],
   "source": [
    "tokenized_sentences = [sent.split() for sent in sentences]\n",
    "print('단어 토큰화 된 결과 :', tokenized_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "총 단어수 : 15\n"
     ]
    }
   ],
   "source": [
    "word_list = []\n",
    "for sent in tokenized_sentences:\n",
    "    for word in sent:\n",
    "        word_list.append(word)\n",
    "word_counts = Counter(word_list)\n",
    "print('총 단어수 :', len(word_counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['nice', 'great', 'best', 'amazing', 'stop', 'lies', 'pitiful', 'nerd', 'excellent', 'work', 'supreme', 'quality', 'bad', 'highly', 'respectable']\n"
     ]
    }
   ],
   "source": [
    "# 등장 빈도순으로 정렬\n",
    "vocab = sorted(word_counts, key=word_counts.get, reverse=True)\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "패딩, UNK 토큰을 고려한 단어 집합의 크기 : 17\n",
      "{'<PAD>': 0, '<UNK>': 1, 'nice': 2, 'great': 3, 'best': 4, 'amazing': 5, 'stop': 6, 'lies': 7, 'pitiful': 8, 'nerd': 9, 'excellent': 10, 'work': 11, 'supreme': 12, 'quality': 13, 'bad': 14, 'highly': 15, 'respectable': 16}\n"
     ]
    }
   ],
   "source": [
    "word_to_index ={}\n",
    "word_to_index['<PAD>'] = 0 \n",
    "word_to_index['<UNK>'] = 1\n",
    "\n",
    "for index, word in enumerate(vocab):\n",
    "    word_to_index[word] = index + 2\n",
    "vocab_size = len(word_to_index)\n",
    "print('패딩, UNK 토큰을 고려한 단어 집합의 크기 :', vocab_size)\n",
    "print(word_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2, 3, 4, 5], [6, 7], [8, 9], [10, 11], [12, 13], [14], [15, 16]]\n"
     ]
    }
   ],
   "source": [
    "def texts_to_sequences(tokenized_X_data, word_to_index):\n",
    "    encoded_X_data = []\n",
    "    for sent in tokenized_X_data:\n",
    "        index_sequences = []\n",
    "        for word in sent:\n",
    "            try:\n",
    "                index_sequences.append(word_to_index[word])\n",
    "            except KeyError:\n",
    "                index_sequences.append(word_to_index['<UNK>'])\n",
    "        encoded_X_data.append(index_sequences)\n",
    "    return encoded_X_data\n",
    "\n",
    "X_encoded = texts_to_sequences(tokenized_sentences, word_to_index)\n",
    "print(X_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "최대 길이 : 4\n"
     ]
    }
   ],
   "source": [
    "# 데이터 최대 길이 측정하고 해당 길이로 패딩 진행\n",
    "max_len = max(len(l) for l in X_encoded)\n",
    "print('최대 길이 :',max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "패딩 결과 :\n",
      "[[ 2  3  4  5]\n",
      " [ 6  7  0  0]\n",
      " [ 8  9  0  0]\n",
      " [10 11  0  0]\n",
      " [12 13  0  0]\n",
      " [14  0  0  0]\n",
      " [15 16  0  0]]\n"
     ]
    }
   ],
   "source": [
    "def pad_sequences(sentences, max_len):\n",
    "    features = np.zeros((len(sentences), max_len), dtype=int)\n",
    "    for index, sentence in enumerate(sentences):\n",
    "        if len(sentence) != 0 :\n",
    "            features[index, :len(sentence)] = np.array(sentence)[:max_len]\n",
    "    return features\n",
    "\n",
    "X_train = pad_sequences(X_encoded, max_len=max_len)\n",
    "y_train = np.array(y_train)\n",
    "print('패딩 결과 :')\n",
    "print(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nn.Embedding() 이용하여 모델 설계\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super(SimpleModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc = nn.Linear(embedding_dim * max_len, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    def forward(self,x):\n",
    "        # embedded.shape == (배치 크기, 문장의 길이, 임베딩 벡터의 차원)\n",
    "        embedded = self.embedding(x)\n",
    "        # flattend.shape == (배치 크기, 문장의 길이 x 임베딩 벡터의 차원)\n",
    "        flattened = self.flatten(embedded)\n",
    "        # output.shape == (배치 크기, 1)\n",
    "        output = self.fc(flattened)\n",
    "        return self.sigmoid(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('mps')\n",
    "embedding_dim = 100 # 임베딩 벡터 100으로 설정\n",
    "simple_model = SimpleModel(vocab_size, embedding_dim).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCELoss() # 출력층이 로지스틱 회귀를 이용한 이진분류 이므로 손실 함수로 바이너리 크로스 엔트로피\n",
    "optimizer = Adam(simple_model.parameters())\n",
    "\n",
    "# 데이터를 배치 크기 2로 설정한 데이터로더로 변환\n",
    "train_dataset = TensorDataset(torch.tensor(X_train, dtype=torch.long), torch.tensor(y_train, dtype=torch.float32))\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "print(len(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 1.0691208839416504\n",
      "Epoch 2, Loss: 0.8290088772773743\n",
      "Epoch 3, Loss: 0.6210417747497559\n",
      "Epoch 4, Loss: 0.4688313901424408\n",
      "Epoch 5, Loss: 0.365804523229599\n",
      "Epoch 6, Loss: 0.2983042895793915\n",
      "Epoch 7, Loss: 0.2539312243461609\n",
      "Epoch 8, Loss: 0.22362205386161804\n",
      "Epoch 9, Loss: 0.2013319581747055\n",
      "Epoch 10, Loss: 0.1832815557718277\n"
     ]
    }
   ],
   "source": [
    "# 10번 학습\n",
    "for epoch in range(10):\n",
    "    for inputs, targets in train_dataloader:\n",
    "        # inputs.shape == (배치 크기, 문장 길이)\n",
    "        # targets.shape == (배치 크기)\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        # outputs.shape == (배치 크기)\n",
    "        outputs = simple_model(inputs).view(-1)\n",
    "\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "    print(f\"Epoch {epoch+1}, Loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gdown\n",
    "!gdown https://drive.google.com/uc?id=1Av37IVBQAAntSe1X3MOAl5gvowQzd2_j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 구글의 사전 훈련된 Word2vec 모델을 로드합니다.\n",
    "word2vec_model = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin.gz', binary=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 엘모(Embeddings from Language Model, ELMo)\n",
    "- 사전 훈련된 언어모델 이용해 임베딩\n",
    "- 같은 단어라도 쓰이는 곳에 따라 전혀 다른 뜻이 됨\n",
    "- Word2Vec, GloVe는 이를 제대로 반영하지 못함\n",
    "- 같은 단어라도 문맥을 고려해 임베딩을 하겠다는 아이디어 -> 문맥을 반영한 워드 임베딩(Contextualized Word Embedding)\n",
    "- biLM(Bidirectional Language Model) -> forward, backward 두개의 언어 모델을 별개의 모델로 보고 학습\n",
    "- 합성곱 신경망을 이용한 문자 임베딩을 통해 얻은 단어 벡터를 입력으로 받음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['medicine', 'best', 'memory', 'Repeat', 'for', 'the', 'is']\n"
     ]
    }
   ],
   "source": [
    "# 단어 단위 RNN\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "sentence = \"Repeat is the best medicine for memory\".split()\n",
    "\n",
    "vocab = list(set(sentence))\n",
    "print(vocab)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'medicine': 1, 'best': 2, 'memory': 3, 'Repeat': 4, 'for': 5, 'the': 6, 'is': 7, '<unk>': 0}\n"
     ]
    }
   ],
   "source": [
    "word2index = {tkn: i for i,tkn in enumerate(vocab, 1)}\n",
    "word2index['<unk>'] = 0\n",
    "print(word2index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 숫자를 단어로 바꾸기 위한 사전\n",
    "index2word = {v:k for k,v in word2index.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_data(sentence, word2index):\n",
    "    encoded = [word2index[token] for token in sentence] # 각 문자를 정수로 변환\n",
    "    input_seq, label_seq = encoded[:-1], encoded[1:] # 입력 시퀀스 라벨 시퀀스 분리\n",
    "    input_seq = torch.LongTensor(input_seq).unsqueeze(0) # 배치 차원 추가\n",
    "    label_seq = torch.LongTensor(label_seq).unsqueeze(0) # 배치 차원 추가\n",
    "    return input_seq, label_seq\n",
    "\n",
    "X, Y = build_data(sentence,word2index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[4, 7, 6, 2, 1, 5]])\n",
      "tensor([[7, 6, 2, 1, 5, 3]])\n"
     ]
    }
   ],
   "source": [
    "print(X)\n",
    "print(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, vocab_size, input_size, hidden_size, batch_first=True):\n",
    "        super(Net, self).__init__()\n",
    "        self.embedding_layer = nn.Embedding(num_embeddings=vocab_size, # 워드 임베딩\n",
    "                                            embedding_dim=input_size)\n",
    "        self.rnn_layer = nn.RNN(input_size, hidden_size, # 입력 차원, 은닉 상태의 크기 정의\n",
    "                                batch_first=batch_first)\n",
    "        self.linear = nn.Linear(hidden_size, vocab_size) # 출력은 원-핫 벡터의 크기를 가져야함. 또는 단어 집합의 크기만큼 가져야함.\n",
    "    def forward(self, x):\n",
    "        # 1. 임베딩 층\n",
    "        # 크기 변화 : (배치 크기, 시퀀스 길이) -> (배치 크기, 시퀀스 길이, 임베딩 차원)\n",
    "        output = self.embedding_layer(x)\n",
    "        # 2. RNN 층\n",
    "        # 크기 변화 : (배치 크기, 시퀀스 길이, 임베딩 차원)\n",
    "        # -> output (배치 크기, 시퀀스 길이, 은닉층 크기), hidden(1, 배치 크기, 은닉층 크기)\n",
    "        output, hidden = self.rnn_layer(output)\n",
    "        # 3. 최종 출력층\n",
    "        # 크기 변화 : (배치 크기, 시퀀스 길이, 은닉층 크기) -> (배치 크기, 시퀀스 길이, 단어장 크기)\n",
    "        output = self.linear(output)\n",
    "        # 4. view를 통해서 배치 차원 제거\n",
    "        # 크기 변화 : (배치 크기, 시퀀스 길이, 단어장 크기) -> (배치 크기 * 시퀀스 길이, 단어장 크기)\n",
    "        return output.view(-1, output.size(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.2072,  0.2207, -0.1984,  0.0843, -0.3080,  0.4439,  0.0447,  0.1445],\n",
      "        [-0.0966,  0.3014,  0.0270,  0.1126, -0.3826, -0.2779,  0.2794,  0.0810],\n",
      "        [ 0.0886,  0.0960, -0.1800,  0.0032, -0.3769,  0.3906,  0.0501,  0.1585],\n",
      "        [ 0.0024,  0.3273,  0.0058,  0.1282, -0.1710,  0.0233,  0.2977,  0.0731],\n",
      "        [ 0.0675,  0.0576, -0.2482,  0.1507, -0.3202,  0.4963, -0.0437,  0.1898],\n",
      "        [-0.0638,  0.1902, -0.2346, -0.1466, -0.4957,  0.3420, -0.2106, -0.0228]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "torch.Size([6, 8])\n"
     ]
    }
   ],
   "source": [
    "# 하이퍼 파라미터\n",
    "vocab_size = len(word2index) # 단어장의 크기는 임베딩 층, 최종 출력층에 사용 <unk> 토큰을 크기에 포함\n",
    "input_size = 5 # 임베딩 된 차원의 크기 및 RNN층 입력 차원의 크기\n",
    "hidden_size = 20 # RNN의 은닉층 크기\n",
    "\n",
    "# 모델 생성\n",
    "model = Net(vocab_size, input_size, hidden_size, batch_first=True)\n",
    "# loss function\n",
    "loss_function = nn.CrossEntropyLoss() # 소프트맥스 함수 포함이며 실제 값은 원-핫 인코딩 안 해도 됨.\n",
    "# 옵티마이져\n",
    "optimizer = optim.Adam(params=model.parameters())\n",
    "\n",
    "output = model(X)\n",
    "print(output) # 현재 가중치 랜덤 초기화 되어있어 의미 있는 예측값 아님\n",
    "print(output.shape) # (시퀀스 길이, 은닉층 크기)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 수치화된 데이터를 단어로 전환하는 함수\n",
    "decode = lambda y : [index2word.get(x) for x in y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[01/201] 1.9771\n",
      "Repeat for medicine for medicine for for\n",
      "\n",
      "[41/201] 1.3146\n",
      "Repeat is the best medicine for memory\n",
      "\n",
      "[81/201] 0.7405\n",
      "Repeat is the best medicine for memory\n",
      "\n",
      "[121/201] 0.3983\n",
      "Repeat is the best medicine for memory\n",
      "\n",
      "[161/201] 0.2262\n",
      "Repeat is the best medicine for memory\n",
      "\n",
      "[201/201] 0.1421\n",
      "Repeat is the best medicine for memory\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 훈련 시작\n",
    "for step in range(201):\n",
    "    # 경사 초기화\n",
    "    optimizer.zero_grad()\n",
    "    # forward\n",
    "    output = model(X)\n",
    "    # loss \n",
    "    loss = loss_function(output, Y.view(-1))\n",
    "    # backward\n",
    "    loss.backward()\n",
    "    # weight update\n",
    "    optimizer.step()\n",
    "    # record\n",
    "    if step % 40 == 0 :\n",
    "        print(f\"[{step+1:02d}/201] {loss:.4f}\")\n",
    "        pred = output.softmax(-1).argmax(-1).tolist()\n",
    "        print(\" \".join(['Repeat'] + decode(pred)))\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
